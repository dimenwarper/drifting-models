# Debug-scaled config — halfway between debug and production
# Goal: replicate production diversity collapse on CPU/Mac
# Larger hidden_dim and batch to stress-test anti-collapse measures

data:
  dataset: "roneneldan/TinyStories"
  split: "train"
  seq_len: 64
  max_samples: 5000
  prompt_conditioning: false

generator:
  hidden_dim: 384          # 3x debug (128), half prod (768) — collapse correlates with dim
  n_layers: 4              # 2x debug, 1/3 prod
  n_heads: 4
  n_style_tokens: 16
  codebook_size: 48

encoder:
  model_name: "gpt2"
  extract_layers: [3, 6, 9, 12]

pooling:
  n_subsample_positions: 16
  window_sizes: [4, 16]

drifting:
  temperatures: [0.02, 0.05, 0.2]
  normalization_momentum: 0.9
  normalize_drift: false
  loss_fn: "huber"
  huber_delta: 1.0

smooth_proj:
  enabled: true
  proj_dim: 256
  hidden_mult: 2
  feature_noise_std: 0.1
  smoothness_weight: 0.01
  lipschitz_noise_std: 0.1

training:
  batch_size: 32             # 2x debug — more samples to collapse into
  n_pos: 32
  queue_size: 1024
  max_steps: 2000            # longer run to see if collapse develops
  diversity_weight: 0.5      # log-barrier
  spectral_weight: 0.01
  vocab_anchor_weight: 1.0
  use_random_neg: false
  n_random_neg: 32
  use_moco_queue: false  # queue negatives cause repulsion imbalance — revert to self-contrastive
  moco_queue_size: 2048
  moco_n_neg: 64
  moco_min_queue: 128
  gradient_accumulation: 1

  optimizer:
    lr: 5.0e-5               # between debug (1e-5) and prod (1e-4)
    betas: [0.9, 0.95]
    weight_decay: 0.05

  scheduler:
    warmup_steps: 100
    min_lr_ratio: 0.01

  gradient_clip: 0.5
  ema_decay: 0.999
  precision: "fp32"

  log_interval: 20
  save_interval: 500
  generate_interval: 200
  generate_n_samples: 4

inference:
  temperature: 1.0
  n_samples: 4
  output_dir: "outputs"
  prompt: "Once upon a time "

paths:
  checkpoint_dir: "checkpoints/debug_scaled"
  log_dir: "logs"

wandb:
  project: "drifting-text"
  enabled: false

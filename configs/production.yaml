# Production training config â€” A100 80GB

data:
  dataset: "roneneldan/TinyStories"
  split: "train"
  seq_len: 256
  max_samples: null
  prompt_conditioning: true
  min_prefix_len: 16
  max_prefix_len: 128

generator:
  hidden_dim: 768
  n_layers: 12
  n_heads: 12
  n_style_tokens: 32
  codebook_size: 64

encoder:
  model_name: "gpt2"
  extract_layers: [3, 6, 9, 12]

pooling:
  n_subsample_positions: 32
  window_sizes: [4, 16]

drifting:
  temperatures: [0.02, 0.05, 0.2]
  normalization_momentum: 0.99

training:
  batch_size: 64
  n_pos: 64
  queue_size: 4096
  max_steps: 200000

  optimizer:
    lr: 1.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.05

  scheduler:
    warmup_steps: 5000
    min_lr_ratio: 0.01

  gradient_clip: 2.0
  ema_decay: 0.999
  precision: "bf16"

  log_interval: 50
  save_interval: 5000
  generate_interval: 2000
  generate_n_samples: 8

inference:
  temperature: 1.0
  n_samples: 16
  output_dir: "outputs"

paths:
  checkpoint_dir: "checkpoints/production"
  log_dir: "logs"

wandb:
  project: "drifting-text"
  enabled: true

# Drifting Models for Text Generation — Default Configuration
# Updated with stability fixes from debug experiments 1-10

# Data
data:
  dataset: "roneneldan/TinyStories"
  split: "train"
  seq_len: 256
  max_samples: null  # null = use all
  prompt_conditioning: true
  min_prefix_len: 16
  max_prefix_len: 128

# Generator
generator:
  hidden_dim: 768
  n_layers: 12
  n_heads: 12
  n_style_tokens: 32
  codebook_size: 64

# Feature encoder
encoder:
  model_name: "gpt2"
  extract_layers: [3, 6, 9, 12]

# Pooling
pooling:
  n_subsample_positions: 32
  window_sizes: [4, 16]

# Drifting
drifting:
  temperatures: [0.02, 0.05, 0.2]
  normalization_momentum: 0.9    # faster tracking (was 0.99, too laggy — exp 3-4)
  normalize_drift: false          # disabled: EMA normalizer makes loss uninformative (exp 4)
  loss_fn: "huber"                # linear grad for large V, no explosion (exp 6)
  huber_delta: 1.0

# Smooth projection — learned feature space regularization (exp 10)
# Projects frozen encoder features to a smooth lower-dim space before drifting loss
smooth_proj:
  enabled: true
  proj_dim: 256
  hidden_mult: 2
  feature_noise_std: 0.1         # noise added to encoder features before projection
  smoothness_weight: 0.01        # Lipschitz penalty weight
  lipschitz_noise_std: 0.1       # perturbation size for smoothness loss

# Training
training:
  batch_size: 64           # N_neg (generated samples per step)
  n_pos: 64                # N_pos (positive samples per step)
  queue_size: 4096
  max_steps: 200000
  gradient_accumulation: 1

  # Optimizer
  optimizer:
    lr: 1.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.05

  # LR schedule
  scheduler:
    warmup_steps: 5000
    min_lr_ratio: 0.01      # min_lr = lr * min_lr_ratio

  diversity_weight: 2.0     # log-barrier is much stronger than 1/x, needs lower weight (was 5.0)
  spectral_weight: 0.01    # soft spectral norm penalty on generator weights
  vocab_anchor_weight: 0.0  # disabled — top-k Gumbel makes vanc ~0 by construction (exp 23)
  position_diversity_weight: 5.0   # intra-sequence position diversity (exp 22-23)
  bigram_diversity_weight: 2.0     # bigram diversity — catches alternating patterns (exp 22-23)
  gumbel_tau: 0.3           # top-k Gumbel-Softmax temperature (exp 23)
  gumbel_topk: 64           # number of nearest vocab tokens per position (exp 23)
  use_random_neg: false       # disabled — replaced by MoCo queue (exp 14)
  n_random_neg: 64           # match batch_size
  use_moco_queue: false       # disabled — queue negatives cause repulsion imbalance (exp 17)
  moco_queue_size: 8192      # ~128 steps of history at batch=64
  moco_n_neg: 128            # negatives sampled per step
  moco_min_queue: 256        # min entries before using queue
  gradient_clip: 0.5        # tight clip needed — grad norms can spike (exp 2)
  ema_decay: 0.999
  precision: "bf16"          # bf16 for GPT-2 forward, fp32 for loss

  # Logging
  log_interval: 50
  save_interval: 5000
  generate_interval: 2000
  generate_n_samples: 8

# Inference
inference:
  temperature: 1.0           # nearest-neighbor temperature (1.0 = argmax)
  n_samples: 16
  output_dir: "outputs"
  prompt: "Once upon a time "

# Paths
paths:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"

# Wandb
wandb:
  project: "drifting-text"
  enabled: false

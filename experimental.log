# Experimental Log — Drifting Models

## 2026-02-09: Anti-collapse + training stability

### Problem
Training collapses — all outputs become repetitive ("CharlesCharlesCharles...") with
vocab_dist/mean=103, std=0.4. Generator converges to a single off-manifold point.
Root cause: when all generated samples are identical, the self-contrastive repulsion
term produces zero gradient (all negatives at the same point). Once collapse begins,
there's no signal to escape.

### Experiment 1: Debug config + diagnostics + diversity regularizer
**Config**: `configs/debug.yaml` (hidden_dim=256, 4 layers, batch=32, 5000 steps)
**Changes**:
- Added `gen_diversity` metric (avg pairwise L2 in gen space) to detect collapse early
- Added `attraction_norm` / `repulsion_norm` split tracking in field.py
- Added diversity regularizer: `1/(avg_dist + 1e-4)` penalty on generator output
- Enhanced logging: per-scale V_norm, feat_scale, min/max scale loss

**Result**: NaN immediately at step 25. All metrics NaN.

**Root cause**: `pairwise_l2` did `dist_sq.clamp(min=0).sqrt()` — gradient of sqrt(x)
at x=0 is infinity. Self-distance diagonal is exactly 0 → NaN gradients everywhere.

**Fix**: `(dist_sq + 1e-8).sqrt()` in field.py.

---

### Experiment 2: Shrink debug config for Mac iteration
**Config**: Reduced to hidden_dim=128, 2 layers, batch=8, seq_len=32, 1000 samples,
500 steps, fp32. Target: run on CPU/Mac for fast iteration.
**Changes**: Also lowered LR (1e-4 → 3e-5), tightened grad clip (2.0 → 0.5).

**Result**: Loss decreases nicely 15→2.8 (steps 10-80), then oscillates and diverges
back to 10-20 by step 200+. Grad norms 100-600x above clip threshold. Attraction
dominates repulsion ~3:1 (attract=40-60, repel=10-20).

**Root cause**: Positive feedback loop — V grows → loss (≈||V||²) grows → large
gradients → generator overshoots → V grows more. DriftNormalizer with momentum=0.99
can't track the growing V fast enough. Also, grad clipping only applied to generator
params, not projection layers (embed_proj, prefix_proj).

**Fixes**:
- Grad clipping now covers all trainable params (generator + projections)
- Still diverged with just that fix.

---

### Experiment 3: V magnitude clamping + faster normalization
**Config**: debug.yaml with `normalization_momentum: 0.9`, `max_V_norm: 5.0`
**Changes**:
- Added `max_V_norm` to DriftingLoss — after drift normalization, clamp any V with
  ||V|| > max_V_norm by scaling it down. Breaks the positive feedback loop.
- Lowered normalization momentum 0.99 → 0.9 so running_lambda tracks faster.
- Added prompt-conditioned generation ("Once upon a time ") for sample logging.

**Result**: Loss stable at ~0.32 (no divergence!), but completely flat — no learning.
V_norm saturates at 4.9-5.0 every step. Grad norms down to 5-25 (much better).
Samples don't improve. Attraction=30-40, repulsion=10-17.

**Root cause**: max_V_norm=5.0 is way too tight. Features are 768-dim, so after drift
normalization the expected ||V|| ≈ sqrt(768) ≈ 28. Clamping to 5 kills the gradient
signal — model gets direction info but effectively zero magnitude, so loss is constant
at ~||V_max||²/C ≈ 0.32.

---

### Experiment 4: Raise max_V_norm + normalizer diagnostics
**Config**: debug.yaml with `max_V_norm: 30.0` (≈ sqrt(768))
**Changes**:
- Raised max_V_norm from 5.0 → 30.0 to match expected normalized V magnitude
- Added `V_raw_norm` metric (pre-normalization V magnitude) to see normalizer effect
- Added `drift_lambda` metric to see what the normalizer is tracking
- Shortened log labels (attract→a, repel→r, diversity→div) to reduce line noise

**Result**: Loss drops 10→2 (steps 10-150), then climbs back to 6-7 by step 360.
Not a divergence blowup — it's λ converging. λ shrinks from 4.9→1.5, inflating
normalized V from 10→22, which inflates loss. V_raw stays flat at 35-40.

**Root cause**: The drift normalizer is working as designed: ||V/λ||²/C → 1. But this
makes the loss ALWAYS ≈ num_scales * 1.0 ≈ 8, regardless of model quality. Loss is
uninformative — it's being normalized away. V_raw staying at 35-40 means the model
isn't actually getting closer to the real distribution.

**Insight**: The normalizer creates a moving target. As model improves (V_raw↓), λ also
shrinks → normalized V stays the same → loss stays the same → gradient magnitude stays
the same. The model never gets a signal that it's improving.

---

### Experiment 5: Disable drift normalization
**Config**: debug.yaml with `normalize_drift: false`, `max_V_norm: 10.0`
**Changes**:
- Added `normalize_drift` flag to DriftingLoss (default true for backward compat)
- Disabled drift normalization in debug config — V = raw field, clamped at 10.0
- max_V_norm=10 prevents divergence (raw V starts at ~40, clamped to manageable range)
- Lambda still tracked for diagnostics but not applied

**Rationale**: Without normalization, the loss directly reflects V magnitude. If the
model improves (gen features approach real distribution), V_raw decreases, loss
decreases — we get a real training signal. The clamp prevents the initial large V from
causing divergence.

**Result**: V pinned at clamp (~9.8-10.0) entire run. V_raw drops 160→35 but stays
well above max_V_norm=10. Loss flat at ~1.28. Same saturation issue as experiment 3
with max_V_norm=5, just at a different constant.

**Root cause**: max_V_norm clamp + MSE is fundamentally broken for this problem:
- Too low → loss constant, no learning signal
- Too high → quadratic MSE causes gradient explosion and divergence
- "Just right" doesn't exist because V_raw varies from 160 (early) to 35 (converged)

---

### Experiment 6: Huber loss — no clamp needed
**Config**: debug.yaml with `loss_fn: "huber"`, `huber_delta: 1.0`, no max_V_norm
**Changes**:
- Added configurable loss function to DriftingLoss: "mse" (default) or "huber"
- Huber loss = quadratic for |V_element| < delta, linear for |V_element| > delta
- For V_raw ≈ 35, C=768: per-element |V| ≈ 35/sqrt(768) ≈ 1.26, right at transition
- Large V → linear loss → bounded gradients → no divergence
- Small V → quadratic loss → fine-grained signal near equilibrium
- No clamp needed — Huber naturally handles the full dynamic range
- Removed max_V_norm from debug config

**Rationale**: MSE(V) = V² makes large V catastrophic (quadratic gradient explosion).
Huber(V) = |V| - δ/2 for large V gives constant-magnitude gradients. As V_raw
decreases during training, loss decreases linearly, giving a real training signal.
When V_raw gets small enough (|V_element| < δ), it transitions to MSE for fine tuning.

**Result**: Best run yet. Three phases:
- Steps 10-80:  loss 4.2→1.5, V_raw 184→47. Genuine learning.
- Steps 80-150: loss 1.5-2.0, V_raw 40-50. Oscillating but holding.
- Steps 150-500: loss slowly climbs 2.0→3.4, V_raw drifts to 50-70.

Grad norms bounded at 3-10 (vs 100-600 in MSE experiments). No NaN, no explosion,
no collapse (diversity 11-17 throughout). Huber loss solved the gradient problem.

Loss climb in phase 3 is likely batch noise: batch_size=8 sees only 0.8% of the 1000
samples per step. Once V_raw is ~40, the per-step signal is small relative to batch
variance — model can't hold its position. vocab_dist stable at 18-21 (vs 103 at
original collapse). Samples still gibberish but diverse, no single-token repetition.

**Verdict**: Training mechanics are sound. Ready to scale up (larger batch, more data).

---

### Summary of fixes applied (experiments 1-6)

| Fix | File | Effect |
|-----|------|--------|
| `(dist_sq + 1e-8).sqrt()` | field.py | Eliminated NaN from sqrt(0) gradient |
| Grad clip on all params | train.py | Prevented unclipped projection gradients |
| Diversity regularizer | train.py | Anti-collapse penalty on generator output |
| Split attract/repel tracking | field.py, loss.py | Diagnose field imbalance |
| V_raw, drift_lambda logging | loss.py, train.py | See normalizer behavior |
| `normalize_drift: false` | loss.py | Avoid EMA lag making loss uninformative |
| Huber loss (`loss_fn: huber`) | loss.py | Linear gradient for large V, no explosion |

---

### Experiment 7: Gradient accumulation (accum=4, batch=8, lr=1e-5)
**Config**: debug.yaml with `gradient_accumulation: 4`, `lr: 1e-5`
**Changes**: Added gradient accumulation to training loop (micro-batch inner loop).
Effective batch=32 (4 × 8). Also lowered LR from 3e-5 to 1e-5.

**Result**: Same pattern. Loss drops 4.5→2.3 (steps 10-150), then climbs back to
4.0+ by step 260. V_raw drops 184→72, then climbs to 88. Grad norms much smoother
(2-15) but the V field still oscillates.

**Root cause**: Gradient accumulation averages the GRADIENTS across micro-batches, but
within each micro-batch, V is still computed on only 8×8 pairwise distances (7 negatives
per sample). The softmax weights over 7 points are inherently unstable — a few outlier
points dominate. The V FIELD is noisy, not just the gradient.

---

### Experiment 8: Larger batch for stable V field
**Config**: debug.yaml with `batch_size: 16`, `n_pos: 16`, `gradient_accumulation: 1`
**Changes**: Increased batch from 8→16, removed gradient accumulation. Now V is
computed on a 16×16 pairwise matrix with 15 negatives per sample (vs 7 before).

**Rationale**: The softmax over 15 negatives is much more stable than over 7. The
pairwise distance matrix has 240 off-diagonal entries (vs 56 with batch=8) — 4x more
information for the field computation. This directly reduces V field variance.

Still fits on Mac: 16 samples × 32 tokens × GPT-2 forward = modest memory.

**Result**: Smoother than batch=8 but same pattern:
- Steps 10-100: loss 4.5→3.6, V_raw 186→163 (slow start)
- Steps 100-230: loss 3.6→1.8, V_raw 163→60 (real learning)
- Steps 230-500: loss climbs 1.8→3.8, V_raw oscillates 50-78

Critical observation at late phase (steps 400-500): LR drops from 1.3e-6→1e-7 (model
nearly frozen), yet V_raw still bounces 56→78→62→72→73→60→75→64→67→66. This is PURE
MEASUREMENT NOISE — the model isn't changing, but each batch gives a different V field.

**Conclusion**: The "climb" is a mix of real drift (steps 280-350 when LR was ~3-5e-6)
and V field batch variance (~±15 V_raw). The raw per-step loss can't distinguish real
degradation from measurement noise. Need EMA-smoothed metrics.

---

### Experiment 9: EMA-smoothed loss metric
**Config**: same as exp 8, but with `ema_loss` tracked in the log
**Changes**: Added EMA-smoothed loss (α=0.05) logged as `ema=` alongside raw loss.
Updated every step (not just at log intervals). This shows the real trend through
the batch noise.

**Result**: EMA confirms the loss climb is real, not just noise:
- Steps 110-230: ema drops 3.72→2.05 (genuine learning)
- Steps 230-310: ema hovers 2.05-2.12 (near-plateau)
- Steps 310-500: ema climbs steadily 2.12→3.32 (real degradation)

**Root cause**: The V field is non-stationary. Unlike supervised learning where the
target is fixed, V depends on the current model (negatives = generated samples). When
the model drifts slightly off-optimal, the negative distribution shifts → V changes →
model drifts further. It's the same feedback loop as the original collapse, just damped
by Huber loss and diversity regularization. Also, the frozen GPT-2 feature space isn't
locally smooth — small perturbations in generator output can cause discontinuous jumps
in encoder features, making V unreliable near the real manifold.

**Informed by**: "Challenges of Diffusion Language Models" blog post (spacehunterinf).
Key parallel: SLD (Segment-Level Diffusion) identifies three essential properties for
latent-space generation: low conversion error, local smoothness, distributional
smoothness. Our frozen GPT-2 features lack local smoothness — not designed for it.

---

### Experiment 10: Smooth projection + noise augmentation
**Config**: debug.yaml with `smooth_proj` enabled
**Changes**:
- Added `SmoothProjectionBank` (src/drifting/smooth_proj.py): learned MLP per feature
  scale that projects pooled encoder features (768/1536-dim) to a lower-dim (256) smooth
  space before the drifting loss
- Noise augmentation: Gaussian noise (σ=0.1) added to encoder features before projection,
  so the V field sees a smoothed version of the landscape
- Lipschitz smoothness penalty: ||proj(x+ε) - proj(x)||²/||ε||² penalizes projections
  that amplify perturbations (weight=0.01)
- Projection MLPs trained jointly with generator via drifting loss gradients

**Rationale**: The frozen GPT-2 feature space has sharp boundaries — small input changes
can cause large feature jumps. The learned projection smooths this out: (1) noise
augmentation prevents the V field from relying on sharp features, (2) the Lipschitz
penalty trains the projection to be contractive, (3) the lower dimensionality (256 vs
768/1536) removes high-frequency noise dimensions. This directly addresses the late-phase
instability where V becomes unreliable near the real manifold.

**Result**: Best stability yet. Dramatically reduced late-phase climb:
- Steps 10-100:  ema 57→45, V_raw 77→61. Warmup phase.
- Steps 100-250: ema 45→12.6, V_raw 61→21. Strong learning (loss drops 4.5x).
- Steps 250-320: ema 12.6-12.9. **Clean plateau** — first time holding steady.
- Steps 320-500: ema 12.9→14.4. Gentle drift (14% from min vs 62% in exp 9).

V_raw stabilized at 22-26 during plateau (vs oscillating 50-80 without smooth proj).
Attract/repel ratio improved to ~3:1 (a=25, r=9) from ~8:1. Smooth loss rock-steady
at 0.012 throughout — Lipschitz constraint learned immediately, no conflict.

Grad norms 30-190 pre-clip (0.5 clip doing heavy work). Diversity healthy at 13-17.
Vocab distances stable ~20 (vs 103 at original collapse).

Samples still gibberish but diverse — expected at this model scale (128-dim, 2 layers).

**Verdict**: Smooth projection solved the late-phase instability. The V field is now
stable enough for the model to hold a plateau. Ready to scale up.

---

### Summary of fixes applied (experiments 1-10)

| Fix | File | Effect |
|-----|------|--------|
| `(dist_sq + 1e-8).sqrt()` | field.py | Eliminated NaN from sqrt(0) gradient |
| Grad clip on all params | train.py | Prevented unclipped projection gradients |
| Diversity regularizer | train.py | Anti-collapse penalty on generator output |
| Split attract/repel tracking | field.py, loss.py | Diagnose field imbalance |
| V_raw, drift_lambda logging | loss.py, train.py | See normalizer behavior |
| `normalize_drift: false` | loss.py | Avoid EMA lag making loss uninformative |
| Huber loss (`loss_fn: huber`) | loss.py | Linear gradient for large V, no explosion |
| EMA-smoothed loss tracking | train.py | See real trend through batch noise |
| Smooth projection + noise aug | smooth_proj.py, train.py | Stable V field near manifold |
| Vocab anchoring loss | train.py | Keep generator output near valid tokens |

---

### Production run (pre-experiment 11): Loss fine, generation collapsed
**Config**: default.yaml (768-dim, 12 layers, batch=64, lr=1e-4, smooth_proj, no vocab anchor)
**Result**: Loss stable at ema~3.0, V_raw~7 at step 25k. BUT generation collapsed:
- "BritishBritishBritish...", "MaryMaryMary...", "PresidentPresidentPresident..."
- vocab_dist/mean = 142 (vs 20 at init) — embeddings far off valid token manifold
- div dropped 88→23 during first 2000 steps (inter-sample diversity declining)

**Root cause**: The drifting loss operates in projected 256-dim space. The model found
solutions that minimize projected feature loss while having degenerate embeddings
(vocab_dist=142 = off-manifold). This is the "rounding error" problem from the
diffusion LM literature — optimizing in latent space doesn't guarantee valid tokens.

---

### Experiment 11: Vocab anchoring loss
**Config**: debug.yaml with `vocab_anchor_weight: 1.0`
**Changes**:
- Added vocab anchoring loss: for each position's embedding, compute min distance to
  any vocab token, penalize the mean. Forces generator to stay near valid tokens.
- `loss += vocab_weight * mean(min_dist_to_vocab(gen_output))`
- Chunked computation option for large vocab (50257 tokens)

**Result**: Vocab anchoring keeps generator on-manifold:
- vanc (mean min-dist to vocab) stable at 17-19 throughout training
- vocab_dist/mean at generated samples: 20→17→19→18.5→18.9 (stable!)
- Compare: without anchoring, vocab_dist climbed to 142 in production
- Loss trajectory similar to exp 10: ema drops 57→12.3, plateau at 12-15
- Repulsion stronger: a/r ratio ~2.5:1 (vs ~3:1 without anchoring)
- Samples at step 500 use real diverse words instead of single-token repetition

**Verdict**: Vocab anchoring solves the generation collapse. Ready for production.

---

### Current debug config (`configs/debug.yaml`)
```
generator: hidden_dim=128, 2 layers, 2 heads
data: seq_len=32, 1000 samples
training: batch=16, n_pos=16, lr=1e-5, grad_clip=0.5, diversity_weight=0.1, vocab_anchor=1.0
drifting: momentum=0.9, normalize_drift=false, loss_fn=huber, huber_delta=1.0
smooth_proj: proj_dim=256, feature_noise=0.1, smoothness_weight=0.01
inference: prompt="Once upon a time "
```

### Key insights
1. V field batch variance is ~±15 V_raw even with a frozen model. With batch=16, the
   per-step loss has a noise floor of ~±1.0. Any trend must be read through this noise
   using smoothing. Raw per-step loss is misleading for training health assessment.
2. Feature space smoothness is critical for stable V fields. Frozen encoder features
   aren't designed for local smoothness — learned projections with Lipschitz constraints
   can fix this (inspired by SLD's approach to latent-space diffusion).
3. The attract/repel ratio is a good health indicator. ~2.5:1 is healthy; ~8:1+ means
   repulsion is too weak to balance attraction (likely due to noisy V field).
4. Loss in projected space can decouple from generation quality. Vocab anchoring is
   essential to keep the generator on the valid token manifold (the "rounding error"
   problem from diffusion LM literature).

---

### Production run (experiment 12): Vocab anchoring works, diversity still collapses
**Config**: default.yaml (768-dim, 12 layers, batch=64, lr=1e-4, vocab_anchor_weight=1.0, diversity_weight=5.0)
**Result**: Vocab anchoring solved the off-manifold problem (vanc dropped 70→2.6, tokens are real words).
BUT diversity still collapsed: div dropped 88→1.5 even with diversity_weight=5.0.
Generated samples: "iiiii...", "is is is...", "when when when..." — valid tokens but all identical.

**Root cause**: The `1/(avg_dist + 1e-4)` diversity regularizer fundamentally fails:
- Wrong gradient shape: ∇(1/x) = -1/x² is too gentle when x is large (diversity healthy)
  and too aggressive when x is tiny (already collapsed, can't recover)
- Critical collapse window (div 88→10) has near-zero penalty — by the time div<1 makes
  penalty spike, model is in a collapse basin it can't escape
- Chicken-and-egg: self-contrastive repulsion needs diversity, diversity needs repulsion.
  When both fail simultaneously, no force can restore diversity.

---

### Deep research: Model collapse in diffusion language models

Literature survey of techniques used in production diffusion LMs to prevent mode collapse:

**1. SDDLM negative gradient regularization** (Semi-autoregressive Discrete Diffusion LM)
- Instead of self-contrastive repulsion (which fails during collapse), sample RANDOM vocab
  tokens as explicit negatives with repulsive gradients
- gradient = -∇_θ score(positive) + E_{neg ~ Uniform(V)} ∇_θ score(negative)
- Random negatives are always diverse regardless of generator state → collapse-independent
- Scales to 1.1B parameters

**2. MoCo-style queue decoupling** (Momentum Contrast for language generation)
- FIFO queue of past generated features as negatives, updated with momentum encoder
- Decouples negative quality from current model state — stale negatives from diverse
  past prevent collapse even as current batch collapses
- Queue size 65536+, momentum decay 0.999

**3. Spectral regularization** (various diffusion model papers)
- Penalize top singular values of generator weight matrices
- Prevents rank collapse where all outputs lie in a low-dim subspace
- Complementary to other approaches

**4. EMA self-distillation** (consistency models, DDIM)
- Use EMA model to generate targets/negatives
- EMA model changes slowly → provides stable, diverse reference
- Already have EMA infrastructure, could be leveraged

**5. Diffusion-LM clamping trick** (Li et al., 2022)
- During decoding, map predicted continuous vectors to nearest word embeddings
- Forces commitment to discrete tokens, prevents drifting into off-manifold space
- Complementary to vocab anchoring (which penalizes distance during training)

**Plan**: Try approach 1 (SDDLM random negatives) first — most targeted fix for our
self-contrastive collapse problem. If that fails, try 5 (clamping trick). If that fails,
try 2 (MoCo queue).

---

### Experiment 13: SDDLM-style random negative repulsion
**Config**: debug.yaml with `use_random_neg: true`, `n_random_neg: 16`
**Changes**:
- Replace self-contrastive negatives with random vocab token sequences
- Each step: sample n_random_neg random token sequences, encode through frozen GPT-2,
  pool features, project through smooth_proj, use as neg_features in drifting loss
- V field becomes: attract(to real) - repel(from random tokens)
- Repulsion is ALWAYS well-defined regardless of generator diversity
- Random tokens represent "where we don't want to be" (incoherent word combinations)
- Keep diversity regularizer as secondary signal

**Result**: Best run yet. No late-phase climb, diversity maintained throughout:
- Steps 10-50:  ema 1.55→1.54, warmup. div 12-15, vanc 17-19.
- Steps 50-250: ema 1.54→1.14, genuine learning. div 11-13.
- Steps 250-500: ema 1.14→1.02, continued improvement (no plateau/climb!). div 10-12.

Attract/repel ratio ~1:1 (vs ~3:1 with self-contrastive) — random negatives provide much
stronger, more consistent repulsion. EMA loss is monotonically decreasing throughout 500
steps — first time we've seen this. No collapse, no divergence, no late-phase degradation.

Generated samples still gibberish (debug scale: 128-dim, 2 layers) but diverse — no
single-token repetition. vocab_dist dropping 20→16 (on-manifold). rneg ~165 (gen far from
random tokens = not producing garbage).

**Verdict**: Random negatives solve the collapse problem. Ready for production test.

---

### Experiment 13 production: Random negatives collapse at scale
**Config**: default.yaml (768-dim, 12 layers, batch=64, use_random_neg=true, n_random_neg=64)
**Result**: div collapsed 36.7→3.6, generated samples repeated single tokens.

**Root cause**: Random negatives push gen away from random tokens but don't push generated
samples apart from **each other**. Two identical outputs get the same repulsive gradient
(same direction away from random tokens) → no inter-sample diversity pressure. The diversity
regularizer 1/(avg_dist + 1e-4) is too weak to counteract this at production scale.

---

### Experiment 14: MoCo queue-based negatives
**Config**: debug.yaml with `use_moco_queue: true`, `moco_queue_size: 1024`, `moco_n_neg: 32`
**Changes**:
- Added `FeatureQueue` (src/drifting/queue.py): multi-scale FIFO queue storing past generated
  features (post smooth_proj) as negatives
- Each step: push current gen_pooled to queue, sample from queue as neg_features
- Past features are from diverse model states → provides inter-sample repulsion even during
  collapse. As long as the queue is large enough (~64+ steps of history), it stays mostly
  diverse through the critical collapse window.
- Falls back to self-contrastive if queue not yet filled (moco_min_queue warmup)
- Queue size logged as `qsz=` in diagnostics
- Disabled use_random_neg (kept code, toggled off)
- Production config: moco_queue_size=8192, moco_n_neg=128, moco_min_queue=256

**Debug result**: Stable at small scale. div held 10-15 throughout 500 steps.

**Production result**: div collapsed 88→5 over 2000 steps. Queue tracked along with collapse —
by step 1000, queue entries were also from collapsed states. MoCo queue can't save you when
the collapse is slow enough that the entire queue history has rotated through.

**Root cause**: No external negatives (self-contrastive, random, MoCo queue) can push collapsed
samples apart — when all generated samples are at the same point, they all receive identical
repulsion vectors. The fix must prevent collapse *structurally*, not through negative strategy.

---

### Experiment 15: Log-barrier diversity + spectral regularization
**Config**: debug.yaml with `diversity_weight: 0.5`, `spectral_weight: 0.01`
**Changes**:
- Replaced diversity regularizer: `1/(avg_dist + 1e-4)` → `-log(avg_dist / D_ref + eps)`
  - D_ref = EMA-tracked reference diversity (momentum 0.99, init from first batch)
  - At div=D_ref: loss = 0 (no penalty at reference level)
  - At div=D_ref/2: loss = 0.69 (30-60x stronger than 1/x at healthy diversity)
  - Gradient -1/x gives consistent pressure across entire diversity range
  - Clamped at 0: no reward for exceeding reference (prevents runaway expansion)
- Added spectral regularization on generator weights (src/drifting/spectral.py)
  - Soft penalty: (sigma_max - 1.0)^2 on weight matrices with dim >= 2, shape[0] >= 4
  - Power iteration (3 iters) for efficient sigma_max approximation
  - Prevents rank collapse where all outputs lie in low-dim subspace
- Lowered diversity_weight: 5.0→2.0 (production), 0.1→0.5 (debug) — log-barrier much stronger
- Added debug_scaled.yaml: halfway config (384-dim, 4 layers, batch=32, 2000 steps)
  to try replicating production collapse on Mac
- Kept MoCo queue enabled — with collapse prevented, queue provides diverse history

**Debug-scaled result** (384-dim, 4 layers, batch=32, 2000 steps, lr=5e-5):

Diversity (PRIMARY GOAL — SUCCEEDED):
- div NEVER collapsed: oscillated 17-40 throughout all 2000 steps
- Actually *increased* in later steps (div=33.6 at step 2000 vs ~19 early)
- dref tracked smoothly 18→32 — log-barrier providing consistent pressure
- Compare to exp 14 production: div 88→5 (catastrophic collapse)

Loss (CONCERNING — late-phase explosion):
- ema started ~2.1 (step 200), oscillated 2-3 mid-run
- Climbed sharply after step 1700: ema=4.65 at step 2000
- V field magnitude grew throughout: V=5-7 early → V=12.0 at end
- λ (attraction weight) climbed 0.50→0.75 — attraction dominating but not converging

Generated samples:
- All gibberish/unicode throughout — never produced English text
- By step 2000: mostly empty strings or unicode garbage (ÃÂÃÂ, rawdownload, etc.)
- Samples remain *diverse* (not collapsed to single token) but not meaningful

Other metrics:
- vanc: dropped 25→2.3 (vocab anchoring working well)
- vocab_dist/mean: dropped 14.7→2.99 (embeddings close to vocab, still generating garbage)
- spectral_reg: stable 12-15 throughout (not visibly helping)
- smooth: stable at 0.011

**Assessment**: Log-barrier diversity is working as designed — collapse is prevented. But the
model isn't learning to generate text. The V field grows monotonically, meaning the generator
moves further from real data over time despite the attraction term. The fundamental learning
signal may need revisiting — the drifting loss successfully prevents collapse but doesn't
guide the generator toward producing coherent text.

**Next steps to investigate**:
1. Loss growing = generator embeddings diverging from real distribution in feature space.
   Is the smooth_proj mapping too lossy? Are we losing the signal that would guide toward
   real text?
2. V growing monotonically suggests attraction and repulsion are both increasing but not
   balancing. May need to investigate the loss landscape geometry.
3. vocab_dist/mean=2.99 means logits are near vocab embeddings, but the *wrong* vocab.
   The nearest-neighbor decoding finds tokens, but they're not meaningful sequences.

---

### Experiment 16: Fix learning signal — FeatureNorm + diversity ratchet + V averaging

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**: Three fixes targeting why V grows monotonically and loss explodes despite
diversity being maintained in exp 15.

**Fix 1: FeatureNormalizer scale from real data only** (PRIMARY)
- Before: `feat_norm.update_scale(cat(phi_gen, phi_pos))` — as gen approaches real,
  cross-distances shrink → scale shrinks → normalized features inflate → V stays large
  regardless of actual convergence. The normalizer directly counteracts learning.
- After: `feat_norm.update_scale(phi_pos)` — real features are stable (frozen GPT-2 +
  fixed dataset), so scale stabilizes. As gen converges, normalized V can actually decrease.

**Fix 2: Freeze diversity reference after init**
- Before: `div_ema_ref = 0.99 * div_ema_ref + 0.01 * avg_dist` — tracked from 18→32 in
  exp 15, a one-way ratchet. Log-barrier defended this elevated floor, pushing samples
  apart and conflicting with drifting loss attraction.
- After: `div_ema_ref` set once from first batch, then frozen. Barrier maintains initial
  diversity level without escalating.

**Fix 3: Average V across temperatures (not sum)**
- Before: V summed across 3 temperatures [0.02, 0.05, 0.2] → V is 3x single-temperature
  magnitude → pushes Huber loss into linear regime (V >> delta=1.0) in most dimensions →
  constant-magnitude gradients, can't distinguish near-equilibrium from far.
- After: V averaged → V magnitude ~1/3 → more dimensions in Huber quadratic regime near
  equilibrium, giving stronger relative gradient signal when close to converging.

**Result**: All three fixes together made things WORSE:
- V grew from 21→55 (vs exp 15's 5→12) — nearly 5x larger
- ema loss climbed to 34 (vs exp 15's 4.65 at worst)
- Samples degraded to single-token repetition
- dref frozen at 41.3 (fix 2 worked as designed)
- div held 20-34 (log-barrier + frozen ref prevented collapse)

**Post-mortem — what went wrong**:

Fix 1 (feat_norm real-only) BACKFIRED: The original combined normalizer (gen+real) acts
as natural damping — when gen is far from real, the scale is large, V is small, training
is gentle. Removing gen from the scale removed this damping, exposing raw V magnitude
(21→55 instead of 5→12). The combined normalizer is a useful stability feature, not a bug.

Fix 2 (frozen div ref) WORKED: dref constant at 41.3 throughout. Diversity held 20-34,
no ratchet. Keep this.

Fix 3 (V averaging) MASKED BY FIX 1: V averaging should reduce V by 3x, but the
feat_norm change inflated V by ~4x, net result was worse. With feat_norm reverted,
V averaging should show its benefit.

---

### Experiment 17: Drop MoCo queue, revert feat_norm, keep good fixes

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

1. **Revert FeatureNormalizer to combined gen+real scale** — the combined scale provides
   natural damping (when gen far from real → large scale → small V → gentle training).
   Exp 16 showed removing this caused V to grow 5x.

2. **Disable MoCo queue → revert to self-contrastive** — queue negatives are structurally
   closer to gen features than real features (they ARE past gen features), causing:
   - Attraction: gen→real distances large → softmax diffuse → weak pull (a=1-5)
   - Repulsion: gen→queue distances small → softmax concentrated → strong push (r=20-55)
   - Result: repulsion 10-20x attraction, generator pushed away from everything.
   Self-contrastive works now because log-barrier + frozen div ref prevent collapse.

3. **Keep V averaging** (from exp 16) — with feat_norm reverted, V should be ~3x lower
   than exp 15's 5→12 range.

4. **Keep frozen div ref** (from exp 16) — confirmed working, prevents ratchet.

**Expected behavior**:
- V should be ~1.5-4 (exp 15 was 5-12, averaging /3, combined normalizer damps further)
- a and r should be comparable magnitude (not 10-20x apart)
- div should stay >10 (log-barrier + frozen ref)
- dref constant after first log
- ema loss should plateau or decrease, NOT climb monotonically
- No rneg or qsz in logs (MoCo disabled)

**Result** (stopped at step 1220 — pattern clear):

Early phase (steps 20-160): Promising. V dropped 25→3.9, ema 14.3→0.69. Best ema minimum
we've seen at this scale. a/r ratio ~3:1 (better than MoCo's 10-20x).

Mid phase (steps 200-500): V grew slowly 5.0→8.2, ema climbed 0.74→1.6. Diversity dropped
from 42 to 12-15, then RATCHETED BACK UP past dref=41.8 (div=26→40 by step 560).
Once div > dref, the log-barrier provides zero pressure (clamped at 0).

Late phase (steps 600-1220): V oscillated 6.2-8.7, ema hovering 1.6-2.0. Diversity
oscillated 32-56 (consistently above dref). Samples degraded catastrophically:
- Step 200: diverse gibberish ("SAMsung sparse", "indignation repetition")
- Step 600: repetitive ("CaliforniaCalifornia", "SomeoneSomeone")
- Step 800: unicode garbage ("�����", empty strings)
- Step 1200: still garbage, vocab_dist/mean dropped 24→6.5

Key metrics at end: V=6.3, ema=1.59, div=36-50, dref=41.8, vanc=3.5, a/r~3:1.
No MoCo metrics (disabled as expected).

**Post-mortem — what went wrong**:

1. **Frozen dref didn't prevent the ratchet** — it prevents the *reference* from ratcheting,
   but the *actual* diversity ratcheted above it. When div > dref, barrier loss = 0 (clamped),
   so the barrier provides no constraint. The diversity expanded freely, driven by
   self-contrastive repulsion with no opposing force.

2. **Self-contrastive repulsion still imbalanced** — a/r ratio was 3:1 throughout, same as
   exp 10/15. The removal of MoCo didn't improve the balance because self-contrastive has the
   same structural problem: gen→gen distances (denominator of repulsion softmax) are always
   smaller than gen→real distances (denominator of attraction softmax). The concentration
   difference creates the imbalance.

3. **Feature-space diversity ≠ text quality** — div=50 means samples are far apart in
   projected feature space, but they decoded to unicode garbage. The drifting loss optimizes
   in feature space, and the generator found a "solution" where embeddings are near vocab
   (vanc=3.5) but map to garbage tokens (ÃÂÃÂ, raw bytes, etc.) that happen to be diverse
   in feature space.

4. **V averaging helped initially but wasn't enough** — V was 3.9-5.0 in early phase (as
   predicted), but still grew to 6-8 later. The combined normalizer's damping effect weakened
   as gen features moved closer to real (scale shrinks → V inflates).

**Key insight**: The fundamental problem is not the negative strategy (MoCo vs self-contrastive
vs random). ALL contrastive approaches push gen away from negatives, but none provide a
gradient toward producing *coherent text*. The attraction term pulls gen toward real features,
but the feature space has many degenerate minima where garbage tokens have similar projected
features to real text. Without a direct token-level signal (like cross-entropy on actual
token predictions), the generator will always find these shortcuts.

---

### Experiment 18: GPT-2 fluency loss — direct token coherence signal

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps, lm_weight=1.0)
**Changes**:

Added GPT-2 fluency loss (LM loss) that gives the generator direct gradient toward
producing coherent token sequences, addressing the fundamental insight from exp 17:
contrastive losses alone can't prevent degenerate minima in feature space.

**How it works**:
1. Generator outputs → embed_proj → frozen GPT-2 → last layer hidden states
2. LM logits = hidden_states @ wte.T (weight-tied LM head, like GPT-2 pretraining)
3. Targets = nearest vocab token for each generated embedding position (via cdist)
4. Loss = shifted cross-entropy: position t predicts position t+1
5. This gives the generator gradient toward sequences where GPT-2 can predict the next
   token — i.e., sequences that follow natural language patterns

**Rationale**: The drifting loss operates in 256-dim projected feature space, where
garbage tokens can have similar features to real text. The LM loss operates at the
token level (50257-dim logits), where garbage tokens get high cross-entropy because
GPT-2 can't predict what comes next. These are complementary signals:
- Drifting loss: "move toward real data distribution in feature space"
- LM loss: "produce sequences where each token is predictable from context"

**Expected behavior**:
- `lm=` should start high (~10, random logits over 50k vocab) and decrease
- If working: samples should show more coherent word sequences
- V and ema metrics should be comparable to exp 17 (same drifting config)
- Combined loss may need weight tuning if lm_loss dominates

**Result** (stopped at step 600 — pattern clear):

Early phase (steps 20-200): LM loss worked spectacularly at first:
- lm dropped 11.31 → 0.09 (100x reduction in 200 steps)
- ema dropped 14.0 → 0.93 (best minimum we've seen at this scale)
- V stable at 5.1-5.3, vanc dropping 27→8.6

Mid phase (steps 200-400): ema started climbing 0.93 → 1.58. LM loss stayed near 0
(0.01-0.23). Samples at step 200 showed repetitive word patterns:
- "bott Moj bott Moj bott" / "JS excerpt JS excerpt" / "Pesh Pesh Pesh"
Diverse between samples but repetitive within each sample.

Late phase (steps 400-600): Samples collapsed to single token per sample:
- "doctr doctr doctr doctr..." (3 out of 4 samples identical)
- ema climbed to 2.0, V grew to 6.7-9.0
- LM loss stayed at 0.02-0.18 (near zero throughout)
- div ratcheted above dref again (48.8 at step 540)

**Post-mortem — LM loss rewards repetition**:

The fundamental problem: minimizing cross-entropy rewards the MOST PREDICTABLE sequences,
and the most predictable sequences are repetitive ones. GPT-2 assigns near-zero cross-entropy
to "doctr doctr doctr..." because after seeing "doctr" 5 times, it predicts "doctr" with
near certainty. The generator found this degenerate minimum instantly.

This is a well-known failure mode:
- Beam search in NMT produces repetitive outputs for the same reason
- Repetition penalty / n-gram blocking is standard in generation for this reason
- Low cross-entropy ≠ natural language; natural language has MODERATE entropy (words are
  somewhat but not perfectly predictable from context)

The LM loss is actively harmful: it pulls the generator toward repetitive sequences,
conflicting with both the drifting loss (which wants gen features to match diverse real
features) and the diversity regularizer.

**Key insight**: The right LM signal is not "minimize cross-entropy" but rather
"produce sequences with GPT-2 perplexity similar to real text." Real text has perplexity
~20-50 on GPT-2, not 1.0 (repetition). Need a target-matching approach, not minimization.

---

### Experiment 19: Perplexity-matching LM loss + repetition penalty

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

Replace the cross-entropy minimization LM loss with two complementary signals:

1. **Perplexity-matching loss**: Instead of minimizing CE, penalize deviation from
   real text perplexity. Compute CE on real text (frozen, computed once), compute CE
   on generated text, loss = (gen_CE - real_CE)^2. This targets the natural entropy
   regime rather than the degenerate minimum at CE=0.

2. **Repetition penalty**: Penalize repeated n-grams in generated token sequences.
   For each position, compute similarity to previous k positions' nearest-vocab tokens.
   If consecutive tokens map to the same vocab token, add penalty. This directly blocks
   the failure mode from exp 18.

**Result** (stopped at step 240 — pattern clear):

Perplexity matching (TECHNICAL SUCCESS):
- Real text CE reference: 2.78 (ppl=16.1)
- gen_ce converged to ~2.76-3.29 range by step 100 — tracking the reference closely
- lm_loss (squared deviation) dropped to ~0.00-0.27 — perplexity matching works perfectly
- Compare exp 18: CE went to 0.01 (repetitive), here it stays at 2.78 (matching real text)

Repetition penalty (FAILED):
- rep_cos climbed 0.55 → 0.92 → 1.00 by step 200 — perfect consecutive repetition
- rep_loss saturated at 0.99-1.00 (penalty too weak, max contribution = 0.5 per step)
- The `enc_cos.clamp(min=0.5).mean()` formulation has a bounded maximum of 1.0,
  providing constant gradient once similarity exceeds 0.5 — not enough to overcome
  the combined pull of drifting loss + LM loss toward repetitive patterns

Generated samples (step 200): Still repetitive word salad with moderate variety:
- "366 depleted 366 lightly 366 366 lightly 366 dwindling 366"
- "opVertDialVert eagleVert eagle op op opVert"
- Better than exp 18's "doctr doctr doctr" — some variety within repetition patterns

Drifting metrics: ema=1.04 at step 200 (better than exp 17's 0.69 but comparable).
V stable at 5.1-5.2. div declining 43→19 (heading toward dref problem again).

**Post-mortem — why auxiliary losses can't fix this**:

The perplexity-matching loss achieved its goal (gen_ce ≈ real_ce) but the generator still
produces garbage. This reveals a deeper issue: **matching aggregate statistics doesn't
guarantee local coherence**. A sequence of random words can have the same average cross-
entropy as natural text if the words are individually common — GPT-2 assigns moderate
probability to many words regardless of context.

The repetition penalty was too weak to overcome the strong gradient from drifting + LM
losses. But even a stronger penalty would just prevent one specific failure mode while
leaving the generator free to find other degenerate minima.

**Key insight**: The problem is that ALL loss signals operate on continuous representations
and are computed per-batch. The generator has enormous freedom to satisfy these constraints
while producing garbage text. Adding more constraints (LM, repetition) adds more
optimization objectives but doesn't reduce the solution space to "coherent text."

The drifting field approach may need a fundamentally different architecture — perhaps
operating at the token level (discrete diffusion) rather than in continuous embedding
space, or fine-tuning GPT-2 instead of using it frozen.

---

### Summary of experiments 15-19: Preventing collapse vs generating text

| Exp | Key Change | Collapse? | Text Quality | ema min | Note |
|-----|-----------|-----------|-------------|---------|------|
| 15 | Log-barrier div + spectral reg | No (div 17-40) | Gibberish/unicode | 2.1 | Diversity maintained, no coherence |
| 16 | feat_norm real-only + frozen dref + V avg | No | Single-token repeat | 34 | feat_norm change broke stability |
| 17 | Drop MoCo, revert feat_norm, keep fixes | No | Unicode garbage | 0.69 | Best ema, worst samples |
| 18 | GPT-2 CE minimization loss | Yes (repetitive) | "doctr doctr doctr" | 0.93 | CE rewards repetition |
| 19 | Perplexity-matching + rep penalty | Partially | Repetitive word salad | 1.04 | PPL match works, rep too weak |

**Fundamental challenge**: Preventing collapse (maintaining diversity) and generating
coherent text are orthogonal problems. Experiments 15-17 solved collapse but produced
garbage; experiments 18-19 added token-level signals but these were gamed by the generator.
The continuous embedding space has too many degenerate solutions that satisfy loss
constraints without producing meaningful text.

---

### Experiment 20: Gumbel-Softmax token snapping

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps, gumbel_tau=1.0→0.1)
**Changes**:

Instead of adding more loss constraints, fundamentally change the generator's output
representation: force it to commit to actual vocab token embeddings using Gumbel-Softmax
before feeding into GPT-2.

**How it works**:
1. Generator outputs → embed_proj → continuous enc_input (B, S, 768)
2. Compute similarities to all vocab tokens: sims = enc_input @ wte.T / tau
3. Gumbel-Softmax: soft_tokens = GumbelSoftmax(sims, hard=False)
4. Snap to vocab: enc_input = soft_tokens @ wte (differentiable weighted sum)
5. Feed snapped embeddings to GPT-2 (now sees near-actual token embeddings)

**Why this should work**:
- Eliminates the "degenerate continuous solution" problem: the generator MUST produce
  distributions over real tokens, not arbitrary embedding vectors
- Features computed on snapped embeddings reflect actual token sequences, not garbage
- As tau anneals from 1.0 (soft, good gradients) to 0.1 (near-hard, actual tokens),
  the generator progressively commits to specific token choices
- The drifting loss now operates on features of real-ish token sequences
- No need for LM loss, repetition penalty, or vocab anchoring — the constraint is
  architectural rather than loss-based

**Disabled**: lm_weight=0, rep_penalty_weight=0 (experiments 18-19 showed these
are gamed by the generator). Kept: drifting loss, diversity reg, spectral reg, smooth_proj.
Vocab anchoring kept (vocab_anchor_weight=1.0) as sanity check but should be redundant
with Gumbel-Softmax.

**Expected behavior**:
- vanc should be very low (embeddings ARE vocab tokens)
- V field should reflect actual token-level differences between gen and real
- Grad norms may be higher (Gumbel-Softmax has discrete-like gradients)
- Samples should show real words (forced by architecture) but may lack coherence
- gtau= shows annealing progress

**Result** (stopped at step 240 — gradient death):

vanc (ARCHITECTURAL SUCCESS): Dropped to 0.0 by step 140. Gumbel-Softmax forces
generator output to exact vocab token embeddings — working as designed.

Gradient death (FUNDAMENTAL FAILURE): grad_norm=0.0 from step 140 onward. The Gumbel-
Softmax over 50k vocab creates near-one-hot distributions because the logit gap between
the nearest token and the rest is huge (cosine similarity to nearest >> to others). The
gradient of a peaked softmax is ~0 for all tokens. The generator is completely frozen.

Loss/V: ema plateaued at 6.2-7.1 (10x higher than experiments without Gumbel). V=13-15
(vs 3-5). a/r ratio extreme: 14:0.6 ≈ 23:1 — repulsion essentially zero.

Generated samples: Real words (forced by architecture) but repetitive word salad:
- "eg eg eg egura eg eg egura eg eg egura"
- "selections selections selections choices Choice revolving"
- No improvement over training (model frozen at step 140)

**Post-mortem — why Gumbel-Softmax fails for large discrete spaces**:

The core issue is that Gumbel-Softmax was designed for small categorical spaces (e.g.,
10-100 categories) where the logit distribution is relatively flat. For 50k vocab:
1. The nearest token has logit >> all others (embedding space is sparse)
2. Softmax saturates to near-one-hot regardless of temperature
3. Gradient through saturated softmax ≈ 0
4. No tau value can fix this: too high → random selection, too low → zero gradient

A top-k approximation (Gumbel-Softmax over k=16 nearest tokens) could help, but adds
complexity and still has the peaked-distribution problem within the top-k.

---

### Meta-analysis: Experiments 15-20 — fundamental architectural limitations

After 6 experiments trying to make the drifting field approach produce coherent text at
the debug-scaled level (384-dim, 4 layers), the pattern is clear:

**What works**:
- Collapse prevention (log-barrier diversity + frozen ref)
- Feature-space matching (V field stable at 3-5 with combined normalizer + V averaging)
- Vocab proximity (vocab anchoring or Gumbel-Softmax keeps embeddings on-manifold)

**What doesn't work**:
- ANY approach to making the features correspond to coherent text
- Auxiliary losses (LM, repetition, perplexity-matching) get gamed
- Architectural constraints (Gumbel-Softmax) kill gradient flow

**Root cause**: Single-pass generation from noise → tokens is fundamentally limited.
The generator sees one noise vector and must produce a complete token sequence that
simultaneously satisfies:
1. Feature-space proximity to real text (drifting loss)
2. Token-level validity (vocab anchoring)
3. Diversity (log-barrier)
4. Spectral regularity

This is too many constraints for a single forward pass. Successful continuous-space text
generation (Diffusion-LM, MDLM, SLD) uses ITERATIVE REFINEMENT:
- Start from noise, take many small steps toward real data
- Each step can use the current state to make informed updates
- The iterative process naturally handles the multi-constraint optimization

**Potential next directions**:
1. Add iterative refinement: multi-step denoising in embedding space (diffusion-style)
2. Fine-tune GPT-2 with drifting loss at the token level (use GPT-2 as generator, not encoder)
3. Abandon continuous generation, use drifting field for discrete token scoring

---

### Experiment 21: Iterative refinement via drifting field

**Motivation**: Exps 15-20 showed single-pass generation can't produce coherent text — the
generator satisfies feature-space constraints while mapping to garbage tokens. Successful
continuous text generation (Diffusion-LM, MDLM) uses iterative refinement. We add K-step
gradient descent in embedding space using the drifting field V as the score function.

**Design**:
```
Generator(noise) → enc_input_0   (initial embeddings)

for k = 0..K-1:
    gen_features_k = GPT-2(enc_input_k)
    gen_pooled_k = pool + smooth_proj(gen_features_k)
    drift_loss_k = drifting_loss(gen_pooled_k, real_pooled)

    if k < K-1:
        grad_k = ∂drift_loss_k / ∂enc_input_k   (create_graph=True)
        enc_input_{k+1} = enc_input_k - η_k * grad_k   (η decays per step)

final loss = drift_loss_{K-1} + diversity + spectral + vocab_anchor(enc_input_{K-1})
```

Key properties:
- Real features computed ONCE, reused across all K steps
- `create_graph=True` keeps computation graph connected — generator gets gradients through
  the full refinement chain
- Refinement gradient clipped to unit norm per step to prevent instability
- Step size decays: η_k = refine_lr × refine_lr_decay^k (coarse→fine)
- Gumbel-Softmax disabled (incompatible — refinement operates on continuous embeddings)
- LM loss and rep penalty disabled (were ineffective in exps 18-20)

**Config** (debug_scaled.yaml):
- n_refine_steps: 3, refine_lr: 0.1, refine_lr_decay: 0.5
- gumbel_tau: 0.0, lm_weight: 0.0, rep_penalty_weight: 0.0

**Expected behavior**:
- refine_0_loss > refine_1_loss > refine_2_loss (loss decreases across refinement steps)
- Refinement gradients non-zero (signal flowing through embedding space)
- Final embeddings closer to real data than initial generator output
- ~2x wall time per step (3 GPT-2 forwards vs 1 for gen side)

**Compute**: 4 GPT-2 forwards/step (1 real + 3 gen), ~7 sec/step, ~4 hrs for 2000 steps.
If OOM: reduce batch_size to 16 or set create_graph=False (loses higher-order gradients).

**Result**: [pending]

---

### Experiment 22: Token-level anti-repetition constraints

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

Experiments 15-21 showed feature-space losses alone can't prevent token repetition. The
generator produces embeddings that match GPT-2 features but map to garbage like "pulling
pulling pulling" or "Cr Cr Cr Boy Girl Cr Cr". The existing repetition penalty (exp 19)
only looks at consecutive pairs — too weak and easily gamed.

Added three token-level constraints:

1. **Position diversity loss** (`position_diversity_weight: 5.0`): Penalize high pairwise
   cosine similarity between ALL position pairs within each sequence. Real text has low avg
   pairwise cosine sim (most tokens differ). "pulling pulling pulling" has sim ≈ 1.0.
   Computed as mean off-diagonal of (B, S, S) cosine similarity matrix.

2. **Bigram diversity loss** (`bigram_diversity_weight: 2.0`): Concatenate consecutive
   positions into bigram vectors (2D), penalize pairwise cosine similarity. Catches "A B A B"
   alternating patterns that unigram diversity misses.

3. **Unique token ratio** (monitoring only, non-differentiable): Fraction of unique nearest-
   neighbor vocab tokens per sequence. Tells us if differentiable losses are working at the
   token level.

Also disabled iterative refinement (`n_refine_steps: 1`) to isolate the new losses.

Starting weights intentionally high — current repetitive output has cosine sim ≈ 0.9, need
strong pressure. If drifting loss stops decreasing, reduce (5.0→2.0, 2.0→1.0).

**Expected behavior**:
- `pdiv` should decrease over training (lower = more diverse positions)
- `bdiv` should decrease (fewer repeated bigrams)
- `uniq` should increase (more unique tokens per sequence)
- Drifting loss should still decrease (not completely dominated by diversity)
- Generated samples should show more variety than "pulling pulling pulling"

**Result**: FAILED. Position/bigram diversity losses had zero effect on repetition.

- pdiv: 0.918 → 0.989 → 0.970 (got WORSE, plateaued near 1.0)
- bdiv: same trajectory as pdiv
- uniq: 0.16 → 0.04 → 0.05 (fewer unique tokens over training)
- ema: 14.3 → 0.77 → 1.70 (dropped then climbed, typical pattern)
- vanc: 25.3 → 5.2 (on-manifold, working)
- Samples at step 400: "Sang Sang Sang Sang", "nipples nipples nipples"

**Post-mortem**: The drifting loss pushes all positions toward a single feature-space
attractor. pdiv/bdiv try to push positions apart but their gradient is diffuse (uniform
average over all 64×64 pairs) while the drifting loss gradient is focused (one direction).
The drifting loss wins. Operating on continuous embeddings where cosine sim is already 0.99
gives pdiv almost no gradient signal to work with.

**Key insight**: pdiv/bdiv need to operate on snapped token embeddings (post-Gumbel) where
repeated tokens have cosine sim = exactly 1.0, and the Gumbel gradient can redirect
positions to different tokens in their top-k set.

---

### Experiment 23: Top-k Gumbel-Softmax + position diversity

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

Exp 20 showed full Gumbel-Softmax (over 50k vocab) kills gradients — the logit gap between
nearest and rest is huge, softmax saturates to one-hot. Exp 22 showed pdiv on continuous
embeddings is ineffective — drifting loss pushes everything to the same point.

**Fix**: Top-k Gumbel-Softmax. Only consider the k=64 nearest vocab tokens per position.

1. **Top-k Gumbel-Softmax** (`gumbel_tau: 1.0, gumbel_topk: 64`): For each position,
   find 64 nearest vocab tokens by dot product. Gumbel-Softmax over those 64 logits —
   distribution is much flatter than over 50k → real gradients flow. Fixed tau (no annealing)
   for simplicity.

2. **Position diversity on post-snap embeddings**: pdiv now operates on the Gumbel-snapped
   embeddings, which are weighted sums of actual vocab tokens. When two positions snap to
   the same token, cosine sim = 1.0, and the Gumbel gradient can push them to different
   tokens in their respective top-k sets.

3. **Vocab anchoring disabled** (`vocab_anchor_weight: 0.0`): Top-k snapping makes
   embeddings near vocab tokens by construction.

4. **Refinement disabled** (`n_refine_steps: 1`): Isolate Gumbel effect.

Also fixed bug: `need_vocab` condition didn't include `gumbel_tau > 0`, so vocab embeddings
weren't moved to device when only Gumbel was enabled.

**Expected behavior**:
- vanc ≈ 0 (embeddings ARE weighted sums of vocab tokens)
- grad_norm > 0 throughout (unlike exp 20's gradient death)
- pdiv should decrease (diversity pressure on actual token embeddings)
- uniq should increase (different positions → different tokens)
- Samples should show diverse real words

**Result**: PARTIAL SUCCESS — gradients alive, diversity improved, but drifting loss can't learn.

Tested two tau values: tau=1.0 (soft) and tau=0.3 (harder).

**tau=1.0 run** (stopped at step 980):

Gradients (SUCCESS — solved exp 20's gradient death):
- grad_norm 35-110 throughout — never zero. Top-k over 64 tokens keeps softmax non-degenerate.
- Compare exp 20 (full 50k Gumbel): grad=0 from step 140 onward.

Position diversity (SUCCESS — pdiv actually decreasing):
- pdiv: 0.698 → 0.395 → 0.331 (steady decrease, contrast exp 22's 0.918 → 0.989)
- bdiv: same trajectory as pdiv
- uniq: 0.43 → 0.30 → 0.25 (started high but declined slowly)

Drifting loss (FAILED — never decreased):
- ema: 9.9 → 8.8 → 10.7 → 11.3 (flat then climbed)
- V: 15-20 throughout (vs 3-5 without Gumbel)
- vocab_dist: 29 → 34 → 41 (embeddings drifting OFF manifold over time)

Samples: Step 200: "27 Reb Blitz Atlantic 27 27 room". Step 400: "flag flag flag Zone flag".
Step 600: "Canadian polar syn polar Canadian". Step 800: "carpet polar rank chord judgment
direction tab rank". Later samples show more variety (~6-7 cycling words) but still repetitive.

**Post-mortem tau=1.0**: Soft Gumbel produces diffuse weighted averages of 64 tokens — not
close to any single token. GPT-2 sees these blurry embeddings and produces features that
don't match real text (V=18 vs V=5 without Gumbel). The drifting loss can't learn because
the input to GPT-2 is fundamentally different from what it was trained on. pdiv and drifting
loss are fighting: diversity pushes toward varied token mixtures, drifting loss needs
coherent single-token embeddings to produce real-like features.

**tau=0.3 run** (stopped at step 600):

Better across the board:
- pdiv: 0.413 → 0.436 → 0.421 (stable ~0.43, not decreasing but started lower)
- uniq: 0.53 → 0.42 → 0.43 (stable ~0.43, much better than tau=1.0's declining 0.25)
- ema: 11.2 → 9.8 → 9.4 (actually decreasing! tau=1.0 was flat/climbing)
- vocab_dist: 27.6 → 28.2 → 26.9 (stable, not drifting off like tau=1.0's 29→41)
- a/r ratio: ~1.7:1 (healthier than tau=1.0's ~2:1)

Samples: Step 200: "b BF Samp Vegas b Cad Cad b BFCE" (~7 words), "positive drivers Stew
positive positive" (~4 words). Step 400: "pole Bride Kay reflection pride fluid Gas freak"
(~8 words). Step 600: "manual precision mus wild monster Dirty Hot animal thread" (~9 words).

**Post-mortem tau=0.3**: Harder snapping means GPT-2 sees more realistic single-token-like
embeddings → drifting loss can learn (ema decreasing). But the model settles into a local
optimum with 6-9 cycling words per sequence. Vocab_dist at 27 suggests embeddings are still
far from real tokens (vs vanc=5 without Gumbel). The 384-dim/4-layer generator may be too
small to solve the harder optimization imposed by top-k Gumbel.

**Comparison across experiments**:

| Metric (@ step 200) | Exp 22 (no Gumbel) | Exp 23 tau=1.0 | Exp 23 tau=0.3 |
|----------------------|--------------------|--------------------|------------|
| pdiv | 0.989 | 0.663 | 0.436 |
| uniq | 0.04 | 0.31 | 0.42 |
| ema | 0.77 | 8.8 | 9.8 |
| vocab_dist | 23.2 | 29.4 | 27.6 |
| grad alive? | yes | yes | yes |

**Key insight**: Top-k Gumbel creates a tau tradeoff:
- High tau (1.0): good diversity gradients, bad drifting loss (blurry embeddings)
- Low tau (0.3): better drifting loss, diversity plateaus at ~0.43
- The fundamental tension: Gumbel snapping adds noise/blur to embeddings that GPT-2 wasn't
  trained on, inflating V and making the drifting loss 10x harder to optimize. The diversity
  gains from architectural token commitment come at the cost of feature-space learning signal.

---

### Experiment 23 production run: Top-k Gumbel at scale (768-dim, 12 layers)

**Config**: default.yaml (768-dim, 12 layers, batch=64, gumbel_tau=0.3, gumbel_topk=64,
position_diversity_weight=5.0, bigram_diversity_weight=2.0, vocab_anchor_weight=0.0)

**Run 1** (without sqrt(D) normalization): GRADIENT DEATH at step 5000.
- At 768-dim, dot products are ~2x larger than 384-dim, making tau=0.3 effectively ~0.15
- Gumbel softmax saturated to one-hot at lr peak → pdiv=1.000, grad=0.0, uniq=0.00
- Fix: Added `/ math.sqrt(D)` to normalize Gumbel logits before softmax

**Run 2** (with sqrt(D) normalization): Survived but plateaued.

Early phase (steps 0-5000):
- ema: dropped from ~10 to ~2.4 (genuine learning)
- pdiv: settled around 0.22 (stable)
- uniq: 0.08-0.17 (low but nonzero)
- div: 88 → 594 → 1166 (exploding past dref=88, log-barrier provides zero constraint)
- vocab_dist: 341 → 3832 (embeddings leaving token manifold)
- Gradients alive throughout, survived lr peak at step 5000

Plateau phase (steps 5000-32000):
- ema: oscillated 2.1-2.9, no improvement over 27k steps
- pdiv: steady 0.22-0.25 (no change)
- uniq: stuck at 0.09-0.16, mostly 0.10-0.12 (actually worse than early phase)
- div: equilibrium around 2000-2500 (not growing further, not shrinking)
- vocab_dist at step 32000: mean=21,229 (!!), std=3,389, max=29,646
  → 5.5x worse than step 10650's 3,832. Embeddings completely off-manifold.
- V: 6.3-9.6, loss: 1.5-3.7 (volatile, not trending down)
- grad: mostly 10-50, occasional spikes to 130-212

Samples at step 10000 (~15 distinct cycling words):
- "Malone massage blanket cannibal suit lighting team spec acle affer Score"
- Some variety but still word salad with heavy repetition

Samples at step 32000 (NO IMPROVEMENT from step 10000):
- "affer lighting cannibal blanket team spec Score acle etting massage"
- Same ~15 words cycling in different orders
- Zero emergence of coherent text after 22k more steps

**Post-mortem — why the run plateaued**:

1. **Vocab distance catastrophe** (vocab_dist 341 → 21,229): The generator found that it can
   minimize the drifting loss in feature space by using a region of embedding space far from
   any real tokens. The Gumbel snap uses whatever's "nearest" but nearest is now 21k units
   away — the snap is meaningless. This is the fundamental failure: nothing forces the
   generator to stay near the token manifold.

2. **No learning signal after plateau**: With embeddings 21k units from vocab, the Gumbel
   snap produces near-random weighted averages of distant tokens. GPT-2 features on these
   inputs are noise → V field is noise → no coherent gradient signal.

3. **Diversity explosion uncontrolled**: div at 2000-2500 vs dref=88 means the log-barrier
   is providing zero constraint. Samples spread out in embedding space but in a meaningless
   way (all far from any real tokens).

4. **pdiv/uniq stagnation**: Position diversity and unique token ratio flatlined because the
   underlying embeddings are so far off-manifold that the Gumbel top-k is just selecting
   among equally irrelevant tokens.

**Key insight**: The Gumbel snap approach has a fatal flaw: it doesn't constrain WHERE in
embedding space the generator operates. The generator can minimize drifting loss while moving
arbitrarily far from the token manifold, making the snap irrelevant. We need an architectural
constraint that keeps generator output ON the token manifold, not just a projection to
nearest tokens after the fact.

**Next experiment**: Unit sphere normalization — project both generator output and vocab
embeddings to the unit sphere before computing Gumbel similarities. This prevents the
generator from "escaping" the token manifold by moving to a distant region of embedding
space, because all directions have unit norm and the only degree of freedom is angle.

---

### Experiment 24: Unit sphere embedding normalization

**Motivation**: Exp 23 production run showed vocab_dist exploding from 341 to 21,229 — the
generator found it can minimize drifting loss by moving embeddings far from ANY vocab token.
The Gumbel snap becomes meaningless when the nearest token is 21k units away.

**Root cause**: The generator output (enc_input) and vocab embeddings live in unbounded
768-dim space. The generator has two degrees of freedom: direction (which token to approximate)
and magnitude (how far from origin). Nothing constrains magnitude, so the generator uses it
to exploit the feature space — moving to regions where GPT-2 features happen to satisfy the
drifting loss but don't correspond to any real tokens.

**Fix**: L2-normalize both generator output and vocab embeddings to the unit sphere before
computing Gumbel similarities. This eliminates the magnitude degree of freedom:
- All embeddings live on the unit hypersphere
- Distance between any two points is bounded [0, 2]
- The only degree of freedom is angular direction → which token(s) to approximate
- vocab_dist is bounded by construction (max = 2.0 on unit sphere)
- Gumbel-Softmax similarities are cosine similarities (dot product of unit vectors)
- No need for sqrt(D) normalization — cosine similarities are already in [-1, 1]

**Design**:
```python
# Before Gumbel snap:
enc_input = F.normalize(enc_input, dim=-1)  # generator output → unit sphere
vocab_norm = F.normalize(vocab_embs_device, dim=-1)  # vocab → unit sphere

# Gumbel snap uses cosine similarity (not raw dot product):
sims = flat @ vocab_norm.T  # cosine sim, range [-1, 1]
# No sqrt(D) needed — already normalized
topk_sims, topk_ids = sims.topk(gumbel_k, dim=-1)
weights = F.gumbel_softmax(topk_sims / gumbel_tau_scale, tau=1.0, hard=False)
# Snap to normalized vocab embeddings:
topk_embs = vocab_norm[topk_ids]
enc_input = (weights.unsqueeze(-1) * topk_embs).sum(dim=1)
# Result: enc_input is ~unit norm (weighted sum of unit vectors)
```

**Key question**: Should we feed NORMALIZED embeddings to GPT-2, or re-scale after snap?
- GPT-2 was trained on wte embeddings which have norm ~1-3 (not unit norm)
- Feeding unit-norm embeddings may change GPT-2's internal dynamics
- Option A: Feed normalized → simpler, GPT-2 may adapt (features still informative)
- Option B: Normalize for snap only, rescale to original vocab norms for GPT-2 forward
  → preserves GPT-2 behavior, slightly more complex
- Start with Option A (simpler), switch to B if features degrade

**Changes**:
- train.py: Add L2 normalization before Gumbel snap, use cosine sim, remove sqrt(D)
- No config changes needed (tau=0.3 should work with cosine sims in [-1,1])

**Expected behavior**:
- vocab_dist should stay bounded (max ~2.0 on unit sphere)
- Gumbel snap should be meaningful (nearest token is always nearby in cosine space)
- pdiv should be comparable or better (cosine sim on unit vectors is cleaner)
- ema should decrease (drifting loss on real-ish embeddings)
- uniq should improve (tokens are real, not distant garbage)
- Samples should show real words that change over training

**Risk**: GPT-2 may produce poor features on unit-norm inputs (different from training
distribution). If so, try Option B (rescale after snap).

**Result**: [superseded by exp 25 before production run completed — exp 24 was pushed but
the production run from exp 23 was killed at step 32k due to plateau. Exp 24's unit sphere
normalization is preserved as part of the cosine similarity computation in exp 25.]

---

### Experiment 25: Simplex snap with temperature annealing (inspired by Categorical Flow Maps)

**Motivation**: Research into ScratchGAN (NeurIPS 2019) and Categorical Flow Maps (Roos et al.,
2026) suggested two key insights:
1. Working on the probability simplex (valid token distributions) prevents manifold escape
   by construction — output is always a convex combination of vocab embeddings
2. Gumbel noise hurts feature quality without providing necessary exploration (the drifting
   field's repulsion provides exploration signal)

**Changes from exp 23/24**:

1. **Remove Gumbel noise**: Replace `gumbel_softmax(logits, tau)` with plain `softmax(logits/tau)`.
   The Gumbel noise added stochasticity that degraded GPT-2 features. The drifting field's
   repulsion term already provides exploration — we don't need sampling noise.

2. **Temperature annealing**: Linear schedule from tau_start=1.0 → tau_end=0.1 over training.
   - Early training (tau=1.0): soft distributions → blurry but smooth optimization landscape
   - Late training (tau=0.1): sharp near-one-hot → realistic token embeddings for GPT-2
   - Provides natural curriculum: coarse → fine-grained learning
   - Inspired by CFM's interpolation on the simplex (soft→hard transition)

3. **Keep cosine similarity** (from exp 24): L2-normalize generator output and vocab embeddings
   before computing similarities. Eliminates magnitude as degree of freedom.

4. **Keep top-k=64**: Prevents softmax saturation over full 50k vocab.

5. **Snap to original (unnormalized) vocab embeddings**: Normalization is only for similarity
   computation. GPT-2 receives embeddings in its native scale.

**Config** (default.yaml):
```yaml
snap_tau_start: 1.0    # start soft
snap_tau_end: 0.1      # anneal to hard
snap_topk: 64          # top-k nearest tokens
```

Replaces: gumbel_tau=0.3, gumbel_topk=64

**Expected behavior**:
- stau= in logs shows annealing from 1.000 → 0.100 over training
- Early: blurry features, high V, but smooth gradients — ema should decrease
- Mid: tau ~0.5, features sharpen, ema should continue decreasing
- Late: tau ~0.1, near-hard token commitment, realistic GPT-2 features
- vocab_dist should stay bounded (convex combination of vocab → max norm ≈ 3)
- No Gumbel noise → deterministic, reproducible, cleaner gradient signal
- pdiv/uniq should improve as tau anneals (sharper → more distinct positions)

**Key question**: Does the soft→hard curriculum give the drifting loss enough signal to learn
in the early blurry phase? tau=1.0 was bad in exp 23 (V=18, ema flat). But that was FIXED tau=1.0
for the whole run. With annealing, the model only spends the first ~10% of training at tau>0.5,
then progressively sharpens. The early blurry phase is just warmup.

**Result**: FAILED — gradient death from step 0. Killed at step 360.

- pdiv: 0.978 → 0.999 → 1.000 (all positions collapsed to identical embeddings)
- uniq: 0.07 → 0.02 (fewer unique tokens over time)
- grad: 0.0-0.2 throughout (generator effectively frozen)
- ema: 11.9 → 7.4 → 9.5 (brief drop from warmup, then climbed)
- V: 14-18 (huge, blurry embeddings — same as exp 23 tau=1.0)
- vocab_dist: 27 (stable but meaningless)
- Samples at step 200: "Berserker Berserker Berserker Berserker bolt Berserker..."

**Post-mortem — plain softmax kills gradients**:

The Gumbel noise was actually essential, not harmful. Without it, plain softmax at high tau
produces nearly uniform weights over 64 tokens (each ≈ 1/64). The softmax gradient
∂weights/∂logits is proportional to weights*(1-weights) — at 1/64 this is ~0.015, giving
near-zero gradient back to the generator. The model is frozen from step 0.

Gumbel noise breaks the symmetry: it creates peaked distributions even at high tau, giving
one or two tokens most of the weight → much larger gradient signal. The stochasticity IS the
gradient — without it, the softmax is too flat to differentiate between good and bad token
choices.

**Key insight**: For top-k softmax over cosine similarities in [-1, 1], the logit range is
too narrow for plain softmax to create peaked distributions at any reasonable temperature.
Gumbel noise effectively expands the logit range, creating the peaked distributions needed
for gradient flow. Removing it was wrong.

---

### Experiment 24 (revised): Unit sphere Gumbel — cosine similarity + Gumbel noise

**Motivation**: Exp 25 showed Gumbel noise is essential for gradient flow. Exp 23 showed
vocab_dist explodes without magnitude constraints. The fix is to combine both:
- L2-normalize for cosine similarity (prevents manifold escape)
- Keep Gumbel-Softmax noise (preserves gradient flow)
- Fixed tau=0.3 (worked well in exp 23 debug-scaled)

This is exactly what exp 24 implemented but never ran — unit sphere normalization with
Gumbel-Softmax preserved. Exp 25 incorrectly removed Gumbel; this reverts to Gumbel while
keeping the cosine similarity fix.

---

### Experiment 26: Hard Gumbel + per-position drift

**Context**: Experiments 20-25 struggled with two coupled problems:
1. **Blurry embeddings**: Soft Gumbel-Softmax (hard=False) produces weighted averages of
   vocab tokens. GPT-2 sees these blurry inputs and produces features that don't match real
   text → V field is noise → no learning signal. Exp 23 production run: ema plateaued,
   vocab_dist exploded to 21k.
2. **Pooling destroys position info**: pool_features() aggregates features across positions
   (subsample 32 random positions, global mean/std, windows of 4/16). The gradient back to
   individual positions is diffuse — every position gets the same update direction → all
   positions converge to same token → repetition. pdiv/bdiv auxiliary losses couldn't
   overcome this (exp 22).

**Changes** (two architectural fixes):

1. **Hard Gumbel-Softmax** (hard=True): Forward uses argmax → actual token embeddings.
   Backward uses soft gradient (straight-through estimator). GPT-2 sees real tokens → V field
   is meaningful. Uses sqrt(D)-scaled dot products (exp 23 — good logit range).

2. **Per-position drift** (per_position_drift: true): Compute V field on raw per-position
   GPT-2 hidden states (B*n, D) instead of pooled multi-scale features. Each position gets
   its own V → naturally prevents repetition without needing pdiv/bdiv auxiliary losses.
   Uses a single configurable layer (drift_layer: 12) with position subsampling
   (n_drift_positions: 32).

3. **Disabled pdiv/bdiv** (position_diversity_weight: 0.0, bigram_diversity_weight: 0.0):
   Per-position drift handles diversity structurally — auxiliary losses are redundant.
   Code kept, just weights zeroed.

**Config**:
- gumbel_tau: 0.3, gumbel_topk: 64 (kept from exp 23)
- per_position_drift: true, drift_layer: 12, n_drift_positions: 32
- position_diversity_weight: 0.0, bigram_diversity_weight: 0.0

**Memory considerations**:
- Pairwise distance matrix: B*n_drift_pos × B*n_drift_pos
  - debug_scaled: 32×32 = 1024 points → 1024×1024 matrix (manageable)
  - production: 64×32 = 2048 points → 2048×2048 matrix (manageable)
- Feature dim: 768→256 after smooth_proj (same as current)

**Expected behavior**:
- `grad` nonzero throughout (straight-through gradients from hard Gumbel)
- `vocab_dist` stays low (hard snap = actual token embeddings)
- `uniq` high and improving (per-position V pushes positions apart)
- `ema` decreasing (meaningful V on real-token features)
- Samples show diverse real words, not single-token repetition
- If V too large (>20): features at 768-dim have larger norms, may need tuning

**Result**: [pending]

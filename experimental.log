# Experimental Log — Drifting Models

## 2026-02-09: Anti-collapse + training stability

### Problem
Training collapses — all outputs become repetitive ("CharlesCharlesCharles...") with
vocab_dist/mean=103, std=0.4. Generator converges to a single off-manifold point.
Root cause: when all generated samples are identical, the self-contrastive repulsion
term produces zero gradient (all negatives at the same point). Once collapse begins,
there's no signal to escape.

### Experiment 1: Debug config + diagnostics + diversity regularizer
**Config**: `configs/debug.yaml` (hidden_dim=256, 4 layers, batch=32, 5000 steps)
**Changes**:
- Added `gen_diversity` metric (avg pairwise L2 in gen space) to detect collapse early
- Added `attraction_norm` / `repulsion_norm` split tracking in field.py
- Added diversity regularizer: `1/(avg_dist + 1e-4)` penalty on generator output
- Enhanced logging: per-scale V_norm, feat_scale, min/max scale loss

**Result**: NaN immediately at step 25. All metrics NaN.

**Root cause**: `pairwise_l2` did `dist_sq.clamp(min=0).sqrt()` — gradient of sqrt(x)
at x=0 is infinity. Self-distance diagonal is exactly 0 → NaN gradients everywhere.

**Fix**: `(dist_sq + 1e-8).sqrt()` in field.py.

---

### Experiment 2: Shrink debug config for Mac iteration
**Config**: Reduced to hidden_dim=128, 2 layers, batch=8, seq_len=32, 1000 samples,
500 steps, fp32. Target: run on CPU/Mac for fast iteration.
**Changes**: Also lowered LR (1e-4 → 3e-5), tightened grad clip (2.0 → 0.5).

**Result**: Loss decreases nicely 15→2.8 (steps 10-80), then oscillates and diverges
back to 10-20 by step 200+. Grad norms 100-600x above clip threshold. Attraction
dominates repulsion ~3:1 (attract=40-60, repel=10-20).

**Root cause**: Positive feedback loop — V grows → loss (≈||V||²) grows → large
gradients → generator overshoots → V grows more. DriftNormalizer with momentum=0.99
can't track the growing V fast enough. Also, grad clipping only applied to generator
params, not projection layers (embed_proj, prefix_proj).

**Fixes**:
- Grad clipping now covers all trainable params (generator + projections)
- Still diverged with just that fix.

---

### Experiment 3: V magnitude clamping + faster normalization
**Config**: debug.yaml with `normalization_momentum: 0.9`, `max_V_norm: 5.0`
**Changes**:
- Added `max_V_norm` to DriftingLoss — after drift normalization, clamp any V with
  ||V|| > max_V_norm by scaling it down. Breaks the positive feedback loop.
- Lowered normalization momentum 0.99 → 0.9 so running_lambda tracks faster.
- Added prompt-conditioned generation ("Once upon a time ") for sample logging.

**Result**: Loss stable at ~0.32 (no divergence!), but completely flat — no learning.
V_norm saturates at 4.9-5.0 every step. Grad norms down to 5-25 (much better).
Samples don't improve. Attraction=30-40, repulsion=10-17.

**Root cause**: max_V_norm=5.0 is way too tight. Features are 768-dim, so after drift
normalization the expected ||V|| ≈ sqrt(768) ≈ 28. Clamping to 5 kills the gradient
signal — model gets direction info but effectively zero magnitude, so loss is constant
at ~||V_max||²/C ≈ 0.32.

---

### Experiment 4: Raise max_V_norm + normalizer diagnostics
**Config**: debug.yaml with `max_V_norm: 30.0` (≈ sqrt(768))
**Changes**:
- Raised max_V_norm from 5.0 → 30.0 to match expected normalized V magnitude
- Added `V_raw_norm` metric (pre-normalization V magnitude) to see normalizer effect
- Added `drift_lambda` metric to see what the normalizer is tracking
- Shortened log labels (attract→a, repel→r, diversity→div) to reduce line noise

**Result**: Loss drops 10→2 (steps 10-150), then climbs back to 6-7 by step 360.
Not a divergence blowup — it's λ converging. λ shrinks from 4.9→1.5, inflating
normalized V from 10→22, which inflates loss. V_raw stays flat at 35-40.

**Root cause**: The drift normalizer is working as designed: ||V/λ||²/C → 1. But this
makes the loss ALWAYS ≈ num_scales * 1.0 ≈ 8, regardless of model quality. Loss is
uninformative — it's being normalized away. V_raw staying at 35-40 means the model
isn't actually getting closer to the real distribution.

**Insight**: The normalizer creates a moving target. As model improves (V_raw↓), λ also
shrinks → normalized V stays the same → loss stays the same → gradient magnitude stays
the same. The model never gets a signal that it's improving.

---

### Experiment 5: Disable drift normalization
**Config**: debug.yaml with `normalize_drift: false`, `max_V_norm: 10.0`
**Changes**:
- Added `normalize_drift` flag to DriftingLoss (default true for backward compat)
- Disabled drift normalization in debug config — V = raw field, clamped at 10.0
- max_V_norm=10 prevents divergence (raw V starts at ~40, clamped to manageable range)
- Lambda still tracked for diagnostics but not applied

**Rationale**: Without normalization, the loss directly reflects V magnitude. If the
model improves (gen features approach real distribution), V_raw decreases, loss
decreases — we get a real training signal. The clamp prevents the initial large V from
causing divergence.

**Result**: V pinned at clamp (~9.8-10.0) entire run. V_raw drops 160→35 but stays
well above max_V_norm=10. Loss flat at ~1.28. Same saturation issue as experiment 3
with max_V_norm=5, just at a different constant.

**Root cause**: max_V_norm clamp + MSE is fundamentally broken for this problem:
- Too low → loss constant, no learning signal
- Too high → quadratic MSE causes gradient explosion and divergence
- "Just right" doesn't exist because V_raw varies from 160 (early) to 35 (converged)

---

### Experiment 6: Huber loss — no clamp needed
**Config**: debug.yaml with `loss_fn: "huber"`, `huber_delta: 1.0`, no max_V_norm
**Changes**:
- Added configurable loss function to DriftingLoss: "mse" (default) or "huber"
- Huber loss = quadratic for |V_element| < delta, linear for |V_element| > delta
- For V_raw ≈ 35, C=768: per-element |V| ≈ 35/sqrt(768) ≈ 1.26, right at transition
- Large V → linear loss → bounded gradients → no divergence
- Small V → quadratic loss → fine-grained signal near equilibrium
- No clamp needed — Huber naturally handles the full dynamic range
- Removed max_V_norm from debug config

**Rationale**: MSE(V) = V² makes large V catastrophic (quadratic gradient explosion).
Huber(V) = |V| - δ/2 for large V gives constant-magnitude gradients. As V_raw
decreases during training, loss decreases linearly, giving a real training signal.
When V_raw gets small enough (|V_element| < δ), it transitions to MSE for fine tuning.

**Result**: Best run yet. Three phases:
- Steps 10-80:  loss 4.2→1.5, V_raw 184→47. Genuine learning.
- Steps 80-150: loss 1.5-2.0, V_raw 40-50. Oscillating but holding.
- Steps 150-500: loss slowly climbs 2.0→3.4, V_raw drifts to 50-70.

Grad norms bounded at 3-10 (vs 100-600 in MSE experiments). No NaN, no explosion,
no collapse (diversity 11-17 throughout). Huber loss solved the gradient problem.

Loss climb in phase 3 is likely batch noise: batch_size=8 sees only 0.8% of the 1000
samples per step. Once V_raw is ~40, the per-step signal is small relative to batch
variance — model can't hold its position. vocab_dist stable at 18-21 (vs 103 at
original collapse). Samples still gibberish but diverse, no single-token repetition.

**Verdict**: Training mechanics are sound. Ready to scale up (larger batch, more data).

---

### Summary of fixes applied (experiments 1-6)

| Fix | File | Effect |
|-----|------|--------|
| `(dist_sq + 1e-8).sqrt()` | field.py | Eliminated NaN from sqrt(0) gradient |
| Grad clip on all params | train.py | Prevented unclipped projection gradients |
| Diversity regularizer | train.py | Anti-collapse penalty on generator output |
| Split attract/repel tracking | field.py, loss.py | Diagnose field imbalance |
| V_raw, drift_lambda logging | loss.py, train.py | See normalizer behavior |
| `normalize_drift: false` | loss.py | Avoid EMA lag making loss uninformative |
| Huber loss (`loss_fn: huber`) | loss.py | Linear gradient for large V, no explosion |

---

### Experiment 7: Gradient accumulation (accum=4, batch=8, lr=1e-5)
**Config**: debug.yaml with `gradient_accumulation: 4`, `lr: 1e-5`
**Changes**: Added gradient accumulation to training loop (micro-batch inner loop).
Effective batch=32 (4 × 8). Also lowered LR from 3e-5 to 1e-5.

**Result**: Same pattern. Loss drops 4.5→2.3 (steps 10-150), then climbs back to
4.0+ by step 260. V_raw drops 184→72, then climbs to 88. Grad norms much smoother
(2-15) but the V field still oscillates.

**Root cause**: Gradient accumulation averages the GRADIENTS across micro-batches, but
within each micro-batch, V is still computed on only 8×8 pairwise distances (7 negatives
per sample). The softmax weights over 7 points are inherently unstable — a few outlier
points dominate. The V FIELD is noisy, not just the gradient.

---

### Experiment 8: Larger batch for stable V field
**Config**: debug.yaml with `batch_size: 16`, `n_pos: 16`, `gradient_accumulation: 1`
**Changes**: Increased batch from 8→16, removed gradient accumulation. Now V is
computed on a 16×16 pairwise matrix with 15 negatives per sample (vs 7 before).

**Rationale**: The softmax over 15 negatives is much more stable than over 7. The
pairwise distance matrix has 240 off-diagonal entries (vs 56 with batch=8) — 4x more
information for the field computation. This directly reduces V field variance.

Still fits on Mac: 16 samples × 32 tokens × GPT-2 forward = modest memory.

**Result**: Smoother than batch=8 but same pattern:
- Steps 10-100: loss 4.5→3.6, V_raw 186→163 (slow start)
- Steps 100-230: loss 3.6→1.8, V_raw 163→60 (real learning)
- Steps 230-500: loss climbs 1.8→3.8, V_raw oscillates 50-78

Critical observation at late phase (steps 400-500): LR drops from 1.3e-6→1e-7 (model
nearly frozen), yet V_raw still bounces 56→78→62→72→73→60→75→64→67→66. This is PURE
MEASUREMENT NOISE — the model isn't changing, but each batch gives a different V field.

**Conclusion**: The "climb" is a mix of real drift (steps 280-350 when LR was ~3-5e-6)
and V field batch variance (~±15 V_raw). The raw per-step loss can't distinguish real
degradation from measurement noise. Need EMA-smoothed metrics.

---

### Experiment 9: EMA-smoothed loss metric
**Config**: same as exp 8, but with `ema_loss` tracked in the log
**Changes**: Added EMA-smoothed loss (α=0.05) logged as `ema=` alongside raw loss.
Updated every step (not just at log intervals). This shows the real trend through
the batch noise.

**Result**: EMA confirms the loss climb is real, not just noise:
- Steps 110-230: ema drops 3.72→2.05 (genuine learning)
- Steps 230-310: ema hovers 2.05-2.12 (near-plateau)
- Steps 310-500: ema climbs steadily 2.12→3.32 (real degradation)

**Root cause**: The V field is non-stationary. Unlike supervised learning where the
target is fixed, V depends on the current model (negatives = generated samples). When
the model drifts slightly off-optimal, the negative distribution shifts → V changes →
model drifts further. It's the same feedback loop as the original collapse, just damped
by Huber loss and diversity regularization. Also, the frozen GPT-2 feature space isn't
locally smooth — small perturbations in generator output can cause discontinuous jumps
in encoder features, making V unreliable near the real manifold.

**Informed by**: "Challenges of Diffusion Language Models" blog post (spacehunterinf).
Key parallel: SLD (Segment-Level Diffusion) identifies three essential properties for
latent-space generation: low conversion error, local smoothness, distributional
smoothness. Our frozen GPT-2 features lack local smoothness — not designed for it.

---

### Experiment 10: Smooth projection + noise augmentation
**Config**: debug.yaml with `smooth_proj` enabled
**Changes**:
- Added `SmoothProjectionBank` (src/drifting/smooth_proj.py): learned MLP per feature
  scale that projects pooled encoder features (768/1536-dim) to a lower-dim (256) smooth
  space before the drifting loss
- Noise augmentation: Gaussian noise (σ=0.1) added to encoder features before projection,
  so the V field sees a smoothed version of the landscape
- Lipschitz smoothness penalty: ||proj(x+ε) - proj(x)||²/||ε||² penalizes projections
  that amplify perturbations (weight=0.01)
- Projection MLPs trained jointly with generator via drifting loss gradients

**Rationale**: The frozen GPT-2 feature space has sharp boundaries — small input changes
can cause large feature jumps. The learned projection smooths this out: (1) noise
augmentation prevents the V field from relying on sharp features, (2) the Lipschitz
penalty trains the projection to be contractive, (3) the lower dimensionality (256 vs
768/1536) removes high-frequency noise dimensions. This directly addresses the late-phase
instability where V becomes unreliable near the real manifold.

**Result**: Best stability yet. Dramatically reduced late-phase climb:
- Steps 10-100:  ema 57→45, V_raw 77→61. Warmup phase.
- Steps 100-250: ema 45→12.6, V_raw 61→21. Strong learning (loss drops 4.5x).
- Steps 250-320: ema 12.6-12.9. **Clean plateau** — first time holding steady.
- Steps 320-500: ema 12.9→14.4. Gentle drift (14% from min vs 62% in exp 9).

V_raw stabilized at 22-26 during plateau (vs oscillating 50-80 without smooth proj).
Attract/repel ratio improved to ~3:1 (a=25, r=9) from ~8:1. Smooth loss rock-steady
at 0.012 throughout — Lipschitz constraint learned immediately, no conflict.

Grad norms 30-190 pre-clip (0.5 clip doing heavy work). Diversity healthy at 13-17.
Vocab distances stable ~20 (vs 103 at original collapse).

Samples still gibberish but diverse — expected at this model scale (128-dim, 2 layers).

**Verdict**: Smooth projection solved the late-phase instability. The V field is now
stable enough for the model to hold a plateau. Ready to scale up.

---

### Summary of fixes applied (experiments 1-10)

| Fix | File | Effect |
|-----|------|--------|
| `(dist_sq + 1e-8).sqrt()` | field.py | Eliminated NaN from sqrt(0) gradient |
| Grad clip on all params | train.py | Prevented unclipped projection gradients |
| Diversity regularizer | train.py | Anti-collapse penalty on generator output |
| Split attract/repel tracking | field.py, loss.py | Diagnose field imbalance |
| V_raw, drift_lambda logging | loss.py, train.py | See normalizer behavior |
| `normalize_drift: false` | loss.py | Avoid EMA lag making loss uninformative |
| Huber loss (`loss_fn: huber`) | loss.py | Linear gradient for large V, no explosion |
| EMA-smoothed loss tracking | train.py | See real trend through batch noise |
| Smooth projection + noise aug | smooth_proj.py, train.py | Stable V field near manifold |
| Vocab anchoring loss | train.py | Keep generator output near valid tokens |

---

### Production run (pre-experiment 11): Loss fine, generation collapsed
**Config**: default.yaml (768-dim, 12 layers, batch=64, lr=1e-4, smooth_proj, no vocab anchor)
**Result**: Loss stable at ema~3.0, V_raw~7 at step 25k. BUT generation collapsed:
- "BritishBritishBritish...", "MaryMaryMary...", "PresidentPresidentPresident..."
- vocab_dist/mean = 142 (vs 20 at init) — embeddings far off valid token manifold
- div dropped 88→23 during first 2000 steps (inter-sample diversity declining)

**Root cause**: The drifting loss operates in projected 256-dim space. The model found
solutions that minimize projected feature loss while having degenerate embeddings
(vocab_dist=142 = off-manifold). This is the "rounding error" problem from the
diffusion LM literature — optimizing in latent space doesn't guarantee valid tokens.

---

### Experiment 11: Vocab anchoring loss
**Config**: debug.yaml with `vocab_anchor_weight: 1.0`
**Changes**:
- Added vocab anchoring loss: for each position's embedding, compute min distance to
  any vocab token, penalize the mean. Forces generator to stay near valid tokens.
- `loss += vocab_weight * mean(min_dist_to_vocab(gen_output))`
- Chunked computation option for large vocab (50257 tokens)

**Result**: Vocab anchoring keeps generator on-manifold:
- vanc (mean min-dist to vocab) stable at 17-19 throughout training
- vocab_dist/mean at generated samples: 20→17→19→18.5→18.9 (stable!)
- Compare: without anchoring, vocab_dist climbed to 142 in production
- Loss trajectory similar to exp 10: ema drops 57→12.3, plateau at 12-15
- Repulsion stronger: a/r ratio ~2.5:1 (vs ~3:1 without anchoring)
- Samples at step 500 use real diverse words instead of single-token repetition

**Verdict**: Vocab anchoring solves the generation collapse. Ready for production.

---

### Current debug config (`configs/debug.yaml`)
```
generator: hidden_dim=128, 2 layers, 2 heads
data: seq_len=32, 1000 samples
training: batch=16, n_pos=16, lr=1e-5, grad_clip=0.5, diversity_weight=0.1, vocab_anchor=1.0
drifting: momentum=0.9, normalize_drift=false, loss_fn=huber, huber_delta=1.0
smooth_proj: proj_dim=256, feature_noise=0.1, smoothness_weight=0.01
inference: prompt="Once upon a time "
```

### Key insights
1. V field batch variance is ~±15 V_raw even with a frozen model. With batch=16, the
   per-step loss has a noise floor of ~±1.0. Any trend must be read through this noise
   using smoothing. Raw per-step loss is misleading for training health assessment.
2. Feature space smoothness is critical for stable V fields. Frozen encoder features
   aren't designed for local smoothness — learned projections with Lipschitz constraints
   can fix this (inspired by SLD's approach to latent-space diffusion).
3. The attract/repel ratio is a good health indicator. ~2.5:1 is healthy; ~8:1+ means
   repulsion is too weak to balance attraction (likely due to noisy V field).
4. Loss in projected space can decouple from generation quality. Vocab anchoring is
   essential to keep the generator on the valid token manifold (the "rounding error"
   problem from diffusion LM literature).

---

### Production run (experiment 12): Vocab anchoring works, diversity still collapses
**Config**: default.yaml (768-dim, 12 layers, batch=64, lr=1e-4, vocab_anchor_weight=1.0, diversity_weight=5.0)
**Result**: Vocab anchoring solved the off-manifold problem (vanc dropped 70→2.6, tokens are real words).
BUT diversity still collapsed: div dropped 88→1.5 even with diversity_weight=5.0.
Generated samples: "iiiii...", "is is is...", "when when when..." — valid tokens but all identical.

**Root cause**: The `1/(avg_dist + 1e-4)` diversity regularizer fundamentally fails:
- Wrong gradient shape: ∇(1/x) = -1/x² is too gentle when x is large (diversity healthy)
  and too aggressive when x is tiny (already collapsed, can't recover)
- Critical collapse window (div 88→10) has near-zero penalty — by the time div<1 makes
  penalty spike, model is in a collapse basin it can't escape
- Chicken-and-egg: self-contrastive repulsion needs diversity, diversity needs repulsion.
  When both fail simultaneously, no force can restore diversity.

---

### Deep research: Model collapse in diffusion language models

Literature survey of techniques used in production diffusion LMs to prevent mode collapse:

**1. SDDLM negative gradient regularization** (Semi-autoregressive Discrete Diffusion LM)
- Instead of self-contrastive repulsion (which fails during collapse), sample RANDOM vocab
  tokens as explicit negatives with repulsive gradients
- gradient = -∇_θ score(positive) + E_{neg ~ Uniform(V)} ∇_θ score(negative)
- Random negatives are always diverse regardless of generator state → collapse-independent
- Scales to 1.1B parameters

**2. MoCo-style queue decoupling** (Momentum Contrast for language generation)
- FIFO queue of past generated features as negatives, updated with momentum encoder
- Decouples negative quality from current model state — stale negatives from diverse
  past prevent collapse even as current batch collapses
- Queue size 65536+, momentum decay 0.999

**3. Spectral regularization** (various diffusion model papers)
- Penalize top singular values of generator weight matrices
- Prevents rank collapse where all outputs lie in a low-dim subspace
- Complementary to other approaches

**4. EMA self-distillation** (consistency models, DDIM)
- Use EMA model to generate targets/negatives
- EMA model changes slowly → provides stable, diverse reference
- Already have EMA infrastructure, could be leveraged

**5. Diffusion-LM clamping trick** (Li et al., 2022)
- During decoding, map predicted continuous vectors to nearest word embeddings
- Forces commitment to discrete tokens, prevents drifting into off-manifold space
- Complementary to vocab anchoring (which penalizes distance during training)

**Plan**: Try approach 1 (SDDLM random negatives) first — most targeted fix for our
self-contrastive collapse problem. If that fails, try 5 (clamping trick). If that fails,
try 2 (MoCo queue).

---

### Experiment 13: SDDLM-style random negative repulsion
**Config**: debug.yaml with `use_random_neg: true`, `n_random_neg: 16`
**Changes**:
- Replace self-contrastive negatives with random vocab token sequences
- Each step: sample n_random_neg random token sequences, encode through frozen GPT-2,
  pool features, project through smooth_proj, use as neg_features in drifting loss
- V field becomes: attract(to real) - repel(from random tokens)
- Repulsion is ALWAYS well-defined regardless of generator diversity
- Random tokens represent "where we don't want to be" (incoherent word combinations)
- Keep diversity regularizer as secondary signal

**Result**: Best run yet. No late-phase climb, diversity maintained throughout:
- Steps 10-50:  ema 1.55→1.54, warmup. div 12-15, vanc 17-19.
- Steps 50-250: ema 1.54→1.14, genuine learning. div 11-13.
- Steps 250-500: ema 1.14→1.02, continued improvement (no plateau/climb!). div 10-12.

Attract/repel ratio ~1:1 (vs ~3:1 with self-contrastive) — random negatives provide much
stronger, more consistent repulsion. EMA loss is monotonically decreasing throughout 500
steps — first time we've seen this. No collapse, no divergence, no late-phase degradation.

Generated samples still gibberish (debug scale: 128-dim, 2 layers) but diverse — no
single-token repetition. vocab_dist dropping 20→16 (on-manifold). rneg ~165 (gen far from
random tokens = not producing garbage).

**Verdict**: Random negatives solve the collapse problem. Ready for production test.

---

### Experiment 13 production: Random negatives collapse at scale
**Config**: default.yaml (768-dim, 12 layers, batch=64, use_random_neg=true, n_random_neg=64)
**Result**: div collapsed 36.7→3.6, generated samples repeated single tokens.

**Root cause**: Random negatives push gen away from random tokens but don't push generated
samples apart from **each other**. Two identical outputs get the same repulsive gradient
(same direction away from random tokens) → no inter-sample diversity pressure. The diversity
regularizer 1/(avg_dist + 1e-4) is too weak to counteract this at production scale.

---

### Experiment 14: MoCo queue-based negatives
**Config**: debug.yaml with `use_moco_queue: true`, `moco_queue_size: 1024`, `moco_n_neg: 32`
**Changes**:
- Added `FeatureQueue` (src/drifting/queue.py): multi-scale FIFO queue storing past generated
  features (post smooth_proj) as negatives
- Each step: push current gen_pooled to queue, sample from queue as neg_features
- Past features are from diverse model states → provides inter-sample repulsion even during
  collapse. As long as the queue is large enough (~64+ steps of history), it stays mostly
  diverse through the critical collapse window.
- Falls back to self-contrastive if queue not yet filled (moco_min_queue warmup)
- Queue size logged as `qsz=` in diagnostics
- Disabled use_random_neg (kept code, toggled off)
- Production config: moco_queue_size=8192, moco_n_neg=128, moco_min_queue=256

**Debug result**: Stable at small scale. div held 10-15 throughout 500 steps.

**Production result**: div collapsed 88→5 over 2000 steps. Queue tracked along with collapse —
by step 1000, queue entries were also from collapsed states. MoCo queue can't save you when
the collapse is slow enough that the entire queue history has rotated through.

**Root cause**: No external negatives (self-contrastive, random, MoCo queue) can push collapsed
samples apart — when all generated samples are at the same point, they all receive identical
repulsion vectors. The fix must prevent collapse *structurally*, not through negative strategy.

---

### Experiment 15: Log-barrier diversity + spectral regularization
**Config**: debug.yaml with `diversity_weight: 0.5`, `spectral_weight: 0.01`
**Changes**:
- Replaced diversity regularizer: `1/(avg_dist + 1e-4)` → `-log(avg_dist / D_ref + eps)`
  - D_ref = EMA-tracked reference diversity (momentum 0.99, init from first batch)
  - At div=D_ref: loss = 0 (no penalty at reference level)
  - At div=D_ref/2: loss = 0.69 (30-60x stronger than 1/x at healthy diversity)
  - Gradient -1/x gives consistent pressure across entire diversity range
  - Clamped at 0: no reward for exceeding reference (prevents runaway expansion)
- Added spectral regularization on generator weights (src/drifting/spectral.py)
  - Soft penalty: (sigma_max - 1.0)^2 on weight matrices with dim >= 2, shape[0] >= 4
  - Power iteration (3 iters) for efficient sigma_max approximation
  - Prevents rank collapse where all outputs lie in low-dim subspace
- Lowered diversity_weight: 5.0→2.0 (production), 0.1→0.5 (debug) — log-barrier much stronger
- Added debug_scaled.yaml: halfway config (384-dim, 4 layers, batch=32, 2000 steps)
  to try replicating production collapse on Mac
- Kept MoCo queue enabled — with collapse prevented, queue provides diverse history

**Debug-scaled result** (384-dim, 4 layers, batch=32, 2000 steps, lr=5e-5):

Diversity (PRIMARY GOAL — SUCCEEDED):
- div NEVER collapsed: oscillated 17-40 throughout all 2000 steps
- Actually *increased* in later steps (div=33.6 at step 2000 vs ~19 early)
- dref tracked smoothly 18→32 — log-barrier providing consistent pressure
- Compare to exp 14 production: div 88→5 (catastrophic collapse)

Loss (CONCERNING — late-phase explosion):
- ema started ~2.1 (step 200), oscillated 2-3 mid-run
- Climbed sharply after step 1700: ema=4.65 at step 2000
- V field magnitude grew throughout: V=5-7 early → V=12.0 at end
- λ (attraction weight) climbed 0.50→0.75 — attraction dominating but not converging

Generated samples:
- All gibberish/unicode throughout — never produced English text
- By step 2000: mostly empty strings or unicode garbage (ÃÂÃÂ, rawdownload, etc.)
- Samples remain *diverse* (not collapsed to single token) but not meaningful

Other metrics:
- vanc: dropped 25→2.3 (vocab anchoring working well)
- vocab_dist/mean: dropped 14.7→2.99 (embeddings close to vocab, still generating garbage)
- spectral_reg: stable 12-15 throughout (not visibly helping)
- smooth: stable at 0.011

**Assessment**: Log-barrier diversity is working as designed — collapse is prevented. But the
model isn't learning to generate text. The V field grows monotonically, meaning the generator
moves further from real data over time despite the attraction term. The fundamental learning
signal may need revisiting — the drifting loss successfully prevents collapse but doesn't
guide the generator toward producing coherent text.

**Next steps to investigate**:
1. Loss growing = generator embeddings diverging from real distribution in feature space.
   Is the smooth_proj mapping too lossy? Are we losing the signal that would guide toward
   real text?
2. V growing monotonically suggests attraction and repulsion are both increasing but not
   balancing. May need to investigate the loss landscape geometry.
3. vocab_dist/mean=2.99 means logits are near vocab embeddings, but the *wrong* vocab.
   The nearest-neighbor decoding finds tokens, but they're not meaningful sequences.

---

### Experiment 16: Fix learning signal — FeatureNorm + diversity ratchet + V averaging

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**: Three fixes targeting why V grows monotonically and loss explodes despite
diversity being maintained in exp 15.

**Fix 1: FeatureNormalizer scale from real data only** (PRIMARY)
- Before: `feat_norm.update_scale(cat(phi_gen, phi_pos))` — as gen approaches real,
  cross-distances shrink → scale shrinks → normalized features inflate → V stays large
  regardless of actual convergence. The normalizer directly counteracts learning.
- After: `feat_norm.update_scale(phi_pos)` — real features are stable (frozen GPT-2 +
  fixed dataset), so scale stabilizes. As gen converges, normalized V can actually decrease.

**Fix 2: Freeze diversity reference after init**
- Before: `div_ema_ref = 0.99 * div_ema_ref + 0.01 * avg_dist` — tracked from 18→32 in
  exp 15, a one-way ratchet. Log-barrier defended this elevated floor, pushing samples
  apart and conflicting with drifting loss attraction.
- After: `div_ema_ref` set once from first batch, then frozen. Barrier maintains initial
  diversity level without escalating.

**Fix 3: Average V across temperatures (not sum)**
- Before: V summed across 3 temperatures [0.02, 0.05, 0.2] → V is 3x single-temperature
  magnitude → pushes Huber loss into linear regime (V >> delta=1.0) in most dimensions →
  constant-magnitude gradients, can't distinguish near-equilibrium from far.
- After: V averaged → V magnitude ~1/3 → more dimensions in Huber quadratic regime near
  equilibrium, giving stronger relative gradient signal when close to converging.

**Result**: All three fixes together made things WORSE:
- V grew from 21→55 (vs exp 15's 5→12) — nearly 5x larger
- ema loss climbed to 34 (vs exp 15's 4.65 at worst)
- Samples degraded to single-token repetition
- dref frozen at 41.3 (fix 2 worked as designed)
- div held 20-34 (log-barrier + frozen ref prevented collapse)

**Post-mortem — what went wrong**:

Fix 1 (feat_norm real-only) BACKFIRED: The original combined normalizer (gen+real) acts
as natural damping — when gen is far from real, the scale is large, V is small, training
is gentle. Removing gen from the scale removed this damping, exposing raw V magnitude
(21→55 instead of 5→12). The combined normalizer is a useful stability feature, not a bug.

Fix 2 (frozen div ref) WORKED: dref constant at 41.3 throughout. Diversity held 20-34,
no ratchet. Keep this.

Fix 3 (V averaging) MASKED BY FIX 1: V averaging should reduce V by 3x, but the
feat_norm change inflated V by ~4x, net result was worse. With feat_norm reverted,
V averaging should show its benefit.

---

### Experiment 17: Drop MoCo queue, revert feat_norm, keep good fixes

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

1. **Revert FeatureNormalizer to combined gen+real scale** — the combined scale provides
   natural damping (when gen far from real → large scale → small V → gentle training).
   Exp 16 showed removing this caused V to grow 5x.

2. **Disable MoCo queue → revert to self-contrastive** — queue negatives are structurally
   closer to gen features than real features (they ARE past gen features), causing:
   - Attraction: gen→real distances large → softmax diffuse → weak pull (a=1-5)
   - Repulsion: gen→queue distances small → softmax concentrated → strong push (r=20-55)
   - Result: repulsion 10-20x attraction, generator pushed away from everything.
   Self-contrastive works now because log-barrier + frozen div ref prevent collapse.

3. **Keep V averaging** (from exp 16) — with feat_norm reverted, V should be ~3x lower
   than exp 15's 5→12 range.

4. **Keep frozen div ref** (from exp 16) — confirmed working, prevents ratchet.

**Expected behavior**:
- V should be ~1.5-4 (exp 15 was 5-12, averaging /3, combined normalizer damps further)
- a and r should be comparable magnitude (not 10-20x apart)
- div should stay >10 (log-barrier + frozen ref)
- dref constant after first log
- ema loss should plateau or decrease, NOT climb monotonically
- No rneg or qsz in logs (MoCo disabled)

**Result** (stopped at step 1220 — pattern clear):

Early phase (steps 20-160): Promising. V dropped 25→3.9, ema 14.3→0.69. Best ema minimum
we've seen at this scale. a/r ratio ~3:1 (better than MoCo's 10-20x).

Mid phase (steps 200-500): V grew slowly 5.0→8.2, ema climbed 0.74→1.6. Diversity dropped
from 42 to 12-15, then RATCHETED BACK UP past dref=41.8 (div=26→40 by step 560).
Once div > dref, the log-barrier provides zero pressure (clamped at 0).

Late phase (steps 600-1220): V oscillated 6.2-8.7, ema hovering 1.6-2.0. Diversity
oscillated 32-56 (consistently above dref). Samples degraded catastrophically:
- Step 200: diverse gibberish ("SAMsung sparse", "indignation repetition")
- Step 600: repetitive ("CaliforniaCalifornia", "SomeoneSomeone")
- Step 800: unicode garbage ("�����", empty strings)
- Step 1200: still garbage, vocab_dist/mean dropped 24→6.5

Key metrics at end: V=6.3, ema=1.59, div=36-50, dref=41.8, vanc=3.5, a/r~3:1.
No MoCo metrics (disabled as expected).

**Post-mortem — what went wrong**:

1. **Frozen dref didn't prevent the ratchet** — it prevents the *reference* from ratcheting,
   but the *actual* diversity ratcheted above it. When div > dref, barrier loss = 0 (clamped),
   so the barrier provides no constraint. The diversity expanded freely, driven by
   self-contrastive repulsion with no opposing force.

2. **Self-contrastive repulsion still imbalanced** — a/r ratio was 3:1 throughout, same as
   exp 10/15. The removal of MoCo didn't improve the balance because self-contrastive has the
   same structural problem: gen→gen distances (denominator of repulsion softmax) are always
   smaller than gen→real distances (denominator of attraction softmax). The concentration
   difference creates the imbalance.

3. **Feature-space diversity ≠ text quality** — div=50 means samples are far apart in
   projected feature space, but they decoded to unicode garbage. The drifting loss optimizes
   in feature space, and the generator found a "solution" where embeddings are near vocab
   (vanc=3.5) but map to garbage tokens (ÃÂÃÂ, raw bytes, etc.) that happen to be diverse
   in feature space.

4. **V averaging helped initially but wasn't enough** — V was 3.9-5.0 in early phase (as
   predicted), but still grew to 6-8 later. The combined normalizer's damping effect weakened
   as gen features moved closer to real (scale shrinks → V inflates).

**Key insight**: The fundamental problem is not the negative strategy (MoCo vs self-contrastive
vs random). ALL contrastive approaches push gen away from negatives, but none provide a
gradient toward producing *coherent text*. The attraction term pulls gen toward real features,
but the feature space has many degenerate minima where garbage tokens have similar projected
features to real text. Without a direct token-level signal (like cross-entropy on actual
token predictions), the generator will always find these shortcuts.

---

### Experiment 18: GPT-2 fluency loss — direct token coherence signal

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps, lm_weight=1.0)
**Changes**:

Added GPT-2 fluency loss (LM loss) that gives the generator direct gradient toward
producing coherent token sequences, addressing the fundamental insight from exp 17:
contrastive losses alone can't prevent degenerate minima in feature space.

**How it works**:
1. Generator outputs → embed_proj → frozen GPT-2 → last layer hidden states
2. LM logits = hidden_states @ wte.T (weight-tied LM head, like GPT-2 pretraining)
3. Targets = nearest vocab token for each generated embedding position (via cdist)
4. Loss = shifted cross-entropy: position t predicts position t+1
5. This gives the generator gradient toward sequences where GPT-2 can predict the next
   token — i.e., sequences that follow natural language patterns

**Rationale**: The drifting loss operates in 256-dim projected feature space, where
garbage tokens can have similar features to real text. The LM loss operates at the
token level (50257-dim logits), where garbage tokens get high cross-entropy because
GPT-2 can't predict what comes next. These are complementary signals:
- Drifting loss: "move toward real data distribution in feature space"
- LM loss: "produce sequences where each token is predictable from context"

**Expected behavior**:
- `lm=` should start high (~10, random logits over 50k vocab) and decrease
- If working: samples should show more coherent word sequences
- V and ema metrics should be comparable to exp 17 (same drifting config)
- Combined loss may need weight tuning if lm_loss dominates

**Result** (stopped at step 600 — pattern clear):

Early phase (steps 20-200): LM loss worked spectacularly at first:
- lm dropped 11.31 → 0.09 (100x reduction in 200 steps)
- ema dropped 14.0 → 0.93 (best minimum we've seen at this scale)
- V stable at 5.1-5.3, vanc dropping 27→8.6

Mid phase (steps 200-400): ema started climbing 0.93 → 1.58. LM loss stayed near 0
(0.01-0.23). Samples at step 200 showed repetitive word patterns:
- "bott Moj bott Moj bott" / "JS excerpt JS excerpt" / "Pesh Pesh Pesh"
Diverse between samples but repetitive within each sample.

Late phase (steps 400-600): Samples collapsed to single token per sample:
- "doctr doctr doctr doctr..." (3 out of 4 samples identical)
- ema climbed to 2.0, V grew to 6.7-9.0
- LM loss stayed at 0.02-0.18 (near zero throughout)
- div ratcheted above dref again (48.8 at step 540)

**Post-mortem — LM loss rewards repetition**:

The fundamental problem: minimizing cross-entropy rewards the MOST PREDICTABLE sequences,
and the most predictable sequences are repetitive ones. GPT-2 assigns near-zero cross-entropy
to "doctr doctr doctr..." because after seeing "doctr" 5 times, it predicts "doctr" with
near certainty. The generator found this degenerate minimum instantly.

This is a well-known failure mode:
- Beam search in NMT produces repetitive outputs for the same reason
- Repetition penalty / n-gram blocking is standard in generation for this reason
- Low cross-entropy ≠ natural language; natural language has MODERATE entropy (words are
  somewhat but not perfectly predictable from context)

The LM loss is actively harmful: it pulls the generator toward repetitive sequences,
conflicting with both the drifting loss (which wants gen features to match diverse real
features) and the diversity regularizer.

**Key insight**: The right LM signal is not "minimize cross-entropy" but rather
"produce sequences with GPT-2 perplexity similar to real text." Real text has perplexity
~20-50 on GPT-2, not 1.0 (repetition). Need a target-matching approach, not minimization.

---

### Experiment 19: Perplexity-matching LM loss + repetition penalty

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps)
**Changes**:

Replace the cross-entropy minimization LM loss with two complementary signals:

1. **Perplexity-matching loss**: Instead of minimizing CE, penalize deviation from
   real text perplexity. Compute CE on real text (frozen, computed once), compute CE
   on generated text, loss = (gen_CE - real_CE)^2. This targets the natural entropy
   regime rather than the degenerate minimum at CE=0.

2. **Repetition penalty**: Penalize repeated n-grams in generated token sequences.
   For each position, compute similarity to previous k positions' nearest-vocab tokens.
   If consecutive tokens map to the same vocab token, add penalty. This directly blocks
   the failure mode from exp 18.

**Result** (stopped at step 240 — pattern clear):

Perplexity matching (TECHNICAL SUCCESS):
- Real text CE reference: 2.78 (ppl=16.1)
- gen_ce converged to ~2.76-3.29 range by step 100 — tracking the reference closely
- lm_loss (squared deviation) dropped to ~0.00-0.27 — perplexity matching works perfectly
- Compare exp 18: CE went to 0.01 (repetitive), here it stays at 2.78 (matching real text)

Repetition penalty (FAILED):
- rep_cos climbed 0.55 → 0.92 → 1.00 by step 200 — perfect consecutive repetition
- rep_loss saturated at 0.99-1.00 (penalty too weak, max contribution = 0.5 per step)
- The `enc_cos.clamp(min=0.5).mean()` formulation has a bounded maximum of 1.0,
  providing constant gradient once similarity exceeds 0.5 — not enough to overcome
  the combined pull of drifting loss + LM loss toward repetitive patterns

Generated samples (step 200): Still repetitive word salad with moderate variety:
- "366 depleted 366 lightly 366 366 lightly 366 dwindling 366"
- "opVertDialVert eagleVert eagle op op opVert"
- Better than exp 18's "doctr doctr doctr" — some variety within repetition patterns

Drifting metrics: ema=1.04 at step 200 (better than exp 17's 0.69 but comparable).
V stable at 5.1-5.2. div declining 43→19 (heading toward dref problem again).

**Post-mortem — why auxiliary losses can't fix this**:

The perplexity-matching loss achieved its goal (gen_ce ≈ real_ce) but the generator still
produces garbage. This reveals a deeper issue: **matching aggregate statistics doesn't
guarantee local coherence**. A sequence of random words can have the same average cross-
entropy as natural text if the words are individually common — GPT-2 assigns moderate
probability to many words regardless of context.

The repetition penalty was too weak to overcome the strong gradient from drifting + LM
losses. But even a stronger penalty would just prevent one specific failure mode while
leaving the generator free to find other degenerate minima.

**Key insight**: The problem is that ALL loss signals operate on continuous representations
and are computed per-batch. The generator has enormous freedom to satisfy these constraints
while producing garbage text. Adding more constraints (LM, repetition) adds more
optimization objectives but doesn't reduce the solution space to "coherent text."

The drifting field approach may need a fundamentally different architecture — perhaps
operating at the token level (discrete diffusion) rather than in continuous embedding
space, or fine-tuning GPT-2 instead of using it frozen.

---

### Summary of experiments 15-19: Preventing collapse vs generating text

| Exp | Key Change | Collapse? | Text Quality | ema min | Note |
|-----|-----------|-----------|-------------|---------|------|
| 15 | Log-barrier div + spectral reg | No (div 17-40) | Gibberish/unicode | 2.1 | Diversity maintained, no coherence |
| 16 | feat_norm real-only + frozen dref + V avg | No | Single-token repeat | 34 | feat_norm change broke stability |
| 17 | Drop MoCo, revert feat_norm, keep fixes | No | Unicode garbage | 0.69 | Best ema, worst samples |
| 18 | GPT-2 CE minimization loss | Yes (repetitive) | "doctr doctr doctr" | 0.93 | CE rewards repetition |
| 19 | Perplexity-matching + rep penalty | Partially | Repetitive word salad | 1.04 | PPL match works, rep too weak |

**Fundamental challenge**: Preventing collapse (maintaining diversity) and generating
coherent text are orthogonal problems. Experiments 15-17 solved collapse but produced
garbage; experiments 18-19 added token-level signals but these were gamed by the generator.
The continuous embedding space has too many degenerate solutions that satisfy loss
constraints without producing meaningful text.

---

### Experiment 20: Gumbel-Softmax token snapping

**Config**: debug_scaled.yaml (384-dim, 4 layers, batch=32, 2000 steps, gumbel_tau=1.0→0.1)
**Changes**:

Instead of adding more loss constraints, fundamentally change the generator's output
representation: force it to commit to actual vocab token embeddings using Gumbel-Softmax
before feeding into GPT-2.

**How it works**:
1. Generator outputs → embed_proj → continuous enc_input (B, S, 768)
2. Compute similarities to all vocab tokens: sims = enc_input @ wte.T / tau
3. Gumbel-Softmax: soft_tokens = GumbelSoftmax(sims, hard=False)
4. Snap to vocab: enc_input = soft_tokens @ wte (differentiable weighted sum)
5. Feed snapped embeddings to GPT-2 (now sees near-actual token embeddings)

**Why this should work**:
- Eliminates the "degenerate continuous solution" problem: the generator MUST produce
  distributions over real tokens, not arbitrary embedding vectors
- Features computed on snapped embeddings reflect actual token sequences, not garbage
- As tau anneals from 1.0 (soft, good gradients) to 0.1 (near-hard, actual tokens),
  the generator progressively commits to specific token choices
- The drifting loss now operates on features of real-ish token sequences
- No need for LM loss, repetition penalty, or vocab anchoring — the constraint is
  architectural rather than loss-based

**Disabled**: lm_weight=0, rep_penalty_weight=0 (experiments 18-19 showed these
are gamed by the generator). Kept: drifting loss, diversity reg, spectral reg, smooth_proj.
Vocab anchoring kept (vocab_anchor_weight=1.0) as sanity check but should be redundant
with Gumbel-Softmax.

**Expected behavior**:
- vanc should be very low (embeddings ARE vocab tokens)
- V field should reflect actual token-level differences between gen and real
- Grad norms may be higher (Gumbel-Softmax has discrete-like gradients)
- Samples should show real words (forced by architecture) but may lack coherence
- gtau= shows annealing progress

**Result** (stopped at step 240 — gradient death):

vanc (ARCHITECTURAL SUCCESS): Dropped to 0.0 by step 140. Gumbel-Softmax forces
generator output to exact vocab token embeddings — working as designed.

Gradient death (FUNDAMENTAL FAILURE): grad_norm=0.0 from step 140 onward. The Gumbel-
Softmax over 50k vocab creates near-one-hot distributions because the logit gap between
the nearest token and the rest is huge (cosine similarity to nearest >> to others). The
gradient of a peaked softmax is ~0 for all tokens. The generator is completely frozen.

Loss/V: ema plateaued at 6.2-7.1 (10x higher than experiments without Gumbel). V=13-15
(vs 3-5). a/r ratio extreme: 14:0.6 ≈ 23:1 — repulsion essentially zero.

Generated samples: Real words (forced by architecture) but repetitive word salad:
- "eg eg eg egura eg eg egura eg eg egura"
- "selections selections selections choices Choice revolving"
- No improvement over training (model frozen at step 140)

**Post-mortem — why Gumbel-Softmax fails for large discrete spaces**:

The core issue is that Gumbel-Softmax was designed for small categorical spaces (e.g.,
10-100 categories) where the logit distribution is relatively flat. For 50k vocab:
1. The nearest token has logit >> all others (embedding space is sparse)
2. Softmax saturates to near-one-hot regardless of temperature
3. Gradient through saturated softmax ≈ 0
4. No tau value can fix this: too high → random selection, too low → zero gradient

A top-k approximation (Gumbel-Softmax over k=16 nearest tokens) could help, but adds
complexity and still has the peaked-distribution problem within the top-k.

---

### Meta-analysis: Experiments 15-20 — fundamental architectural limitations

After 6 experiments trying to make the drifting field approach produce coherent text at
the debug-scaled level (384-dim, 4 layers), the pattern is clear:

**What works**:
- Collapse prevention (log-barrier diversity + frozen ref)
- Feature-space matching (V field stable at 3-5 with combined normalizer + V averaging)
- Vocab proximity (vocab anchoring or Gumbel-Softmax keeps embeddings on-manifold)

**What doesn't work**:
- ANY approach to making the features correspond to coherent text
- Auxiliary losses (LM, repetition, perplexity-matching) get gamed
- Architectural constraints (Gumbel-Softmax) kill gradient flow

**Root cause**: Single-pass generation from noise → tokens is fundamentally limited.
The generator sees one noise vector and must produce a complete token sequence that
simultaneously satisfies:
1. Feature-space proximity to real text (drifting loss)
2. Token-level validity (vocab anchoring)
3. Diversity (log-barrier)
4. Spectral regularity

This is too many constraints for a single forward pass. Successful continuous-space text
generation (Diffusion-LM, MDLM, SLD) uses ITERATIVE REFINEMENT:
- Start from noise, take many small steps toward real data
- Each step can use the current state to make informed updates
- The iterative process naturally handles the multi-constraint optimization

**Potential next directions**:
1. Add iterative refinement: multi-step denoising in embedding space (diffusion-style)
2. Fine-tune GPT-2 with drifting loss at the token level (use GPT-2 as generator, not encoder)
3. Abandon continuous generation, use drifting field for discrete token scoring
